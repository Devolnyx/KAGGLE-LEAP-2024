{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "Import all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import librosa\n",
    "from scipy import signal as sci_signal\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "\n",
    "# import score function of BirdCLEF\n",
    "#sys.path.append('/kaggle/input/birdclef-roc-auc')\n",
    "#sys.path.append('/kaggle/usr/lib/kaggle_metric_utilities')\n",
    "#from metric import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vyacheslav\\AppData\\Local\\Temp\\ipykernel_12076\\1764714236.py:3: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = mpl.cm.get_cmap('coolwarm')\n"
     ]
    }
   ],
   "source": [
    "# Import for visualization\n",
    "import matplotlib as mpl\n",
    "cmap = mpl.cm.get_cmap('coolwarm')\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display as lid\n",
    "import IPython.display as ipd\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.674293518066406"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saturated steam\n",
    "\n",
    "T = torch.tensor(25) #Ceslium\n",
    "\n",
    "Es = 6.112*torch.e**(17.67*T/(T + 243.5)) # 'hectopascal'\n",
    "Es.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.383702343092022"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mixing ratio\n",
    "q = 1.484e-06 #kg/kg\n",
    "p = 1000  #units.hPa\n",
    "\n",
    "q = q * 1000\n",
    "w = q / (1-q)\n",
    "\n",
    "# Water partial pressure\n",
    "e = w / (0.622 + w) * p\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.525668541691085"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relative humidity\n",
    "100 * e / Es.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Hyper-paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 28082015\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    \n",
    "    # == global config ==\n",
    "    SEED = 28082015  # random seed\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # device to be used\n",
    "    MIXED_PRECISION = False  # whether to use mixed-16 precision\n",
    "    OUTPUT_DIR = './output/'  # output folder\n",
    "    \n",
    "    # == data config ==\n",
    "    DATA_ROOT = 'E:/PycharmProjects/birdclef24/data'  # root folder\n",
    "    PREPROCESSED_DATA_ROOT = '/kaggle/input/birdclef24-spectrograms-via-cupy'\n",
    "    LOAD_DATA = True  # whether to load data from pre-processed dataset\n",
    "\n",
    "    \n",
    "    # == model config ==\n",
    "    MODEL_TYPE = 'efficientnet_b0'  # model type\n",
    "    \n",
    "    # == dataset config ==\n",
    "    BATCH_SIZE = 256  # batch size of each step\n",
    "    N_WORKERS = 6  # number of workers\n",
    "    \n",
    "    \n",
    "    # == training config ==\n",
    "    FOLDS = 7  # n fold\n",
    "    EPOCHS = 200  # max epochs\n",
    "    LR = 7e-4  # learning rate\n",
    "    WEIGHT_DECAY = 9e-6  # weight decay of optimizer\n",
    "    \n",
    "    # == other config ==\n",
    "    VISUALIZE = True  # whether to visualize data and batch\n",
    "    \n",
    "    \n",
    "print('fix seed')\n",
    "pl.seed_everything(config.SEED, workers=True)\n",
    "\n",
    "CFG = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA(nn.Module):\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.supports_masking = True\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=kernel_size, stride=1, padding=\"same\", bias=False)\n",
    "    def forward(self, inputs):\n",
    "        b, c, s = inputs.shape\n",
    "        \n",
    "        x = torch.mean(inputs, axis = -1)\n",
    "        x = x.view(b, 1, c)\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = x[:,:,None]\n",
    "        return inputs * x\n",
    "\n",
    "\n",
    "class CausalDWConv1D(nn.Module):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        in_channels = 64,\n",
    "        out_channels = 32,       \n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        **kwargs):\n",
    "        super().__init__()\n",
    "        #self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=1, \n",
    "            padding='same', \n",
    "            dilation=dilation_rate, \n",
    "            groups=out_channels if kernel_size > 3 else 1,\n",
    "            bias=False, \n",
    "            padding_mode='zeros')\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.dw_conv(inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1DBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 kernel_size=17,\n",
    "                 channels = 32,\n",
    "                 expand_channels = 64,\n",
    "                 drop_rate=0.0,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = CausalDWConv1D(\n",
    "                        kernel_size=kernel_size,\n",
    "                        dilation_rate=1,\n",
    "                        use_bias=False,\n",
    "                        in_channels = expand_channels,\n",
    "                        out_channels = expand_channels\n",
    "                    )\n",
    "        self.dnn_expand = nn.Linear(in_features = channels, \n",
    "                                    out_features = expand_channels\n",
    "                                     )\n",
    "        self.dnn_project = nn.Linear(in_features = expand_channels, \n",
    "                             out_features = channels\n",
    "                                    )\n",
    "        self.bn = nn.BatchNorm1d(num_features = expand_channels, eps=0.95)\n",
    "        self.eca = ECA()\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        skip = inputs\n",
    "\n",
    "        x = inputs.permute([0,2,1])\n",
    "        x = self.dnn_expand(x)\n",
    "        \n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.act(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.eca(x)\n",
    "        \n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.dnn_project(x)\n",
    "        x = x.permute([0,2,1])\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class Conv1DModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 kernel_size=17,\n",
    "                 channels = 32,\n",
    "                 expand_channels = 64,\n",
    "                 drop_rate=0.0,\n",
    "                 num_blocks_in_stage = 3,\n",
    "                 input_len = 32_000*5,\n",
    "                 n_classes = 182\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.stem_conv = nn.Linear(in_features = 1, \n",
    "                                    out_features = channels\n",
    "                                     )\n",
    "        self.stem_bn = nn.BatchNorm1d(num_features = channels, eps=0.95)\n",
    "\n",
    "        self.ConvStage_1 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                         for _ in range(num_blocks_in_stage)])\n",
    "        self.PoolStage_1 = nn.AvgPool1d(kernel_size=(4))\n",
    "        \n",
    "        self.ConvStage_2 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                          for _ in range(num_blocks_in_stage)])\n",
    "        self.PoolStage_2 = nn.AvgPool1d(kernel_size=(4))\n",
    "\n",
    "        \n",
    "        self.ConvStage_3 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                          for _ in range(num_blocks_in_stage)])\n",
    "        self.PoolStage_3 = nn.AvgPool1d(kernel_size=(4))\n",
    "\n",
    "        self.pre_out = nn.Linear(in_features = channels, out_features = n_classes*2)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.out_act = nn.SiLU()\n",
    "        self.out = nn.Linear(in_features = n_classes*2, out_features = n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        b, s = inputs.shape\n",
    "        x = inputs.view(b, s, 1)\n",
    "        x = self.stem_conv(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.stem_bn(x)\n",
    "\n",
    "        for block in self.ConvStage_1:\n",
    "            x = block(x)\n",
    "        x = self.PoolStage_1(x)\n",
    "\n",
    "        for block in self.ConvStage_2:\n",
    "            x = block(x)\n",
    "        x = self.PoolStage_2(x)\n",
    "\n",
    "        for block in self.ConvStage_3:\n",
    "            x = block(x)\n",
    "        x = self.PoolStage_3(x)\n",
    "\n",
    "        x = x.mean(axis=2)\n",
    "\n",
    "        x = self.pre_out(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_act(x)\n",
    "        \n",
    "        logits = self.out(x)\n",
    "        probs = self.sigmoid(logits)\n",
    "\n",
    "        return {\n",
    "                \"clipwise_logits_long\": logits,\n",
    "                \"clipwise_pred_long\": probs,\n",
    "            }\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = nn.MultiheadAttention(n_embd, n_head)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, q = None):\n",
    "        if q is not None:\n",
    "            X = (q, x, x)\n",
    "        else:\n",
    "            X = (x, x, x)\n",
    "        y = self.sa(*X)\n",
    "        y = y[0]\n",
    "        \n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransBlock(nn.Module):\n",
    "    def __init__(self, block_kernels = [5, 3], n_head = 4, channels=16, expand_channels=32, drop_rate = 0.1, att_drop_rate = 0.25, n_features=25):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(*[\n",
    "            Conv1DBlock(kernel_size=k, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "            for k in block_kernels\n",
    "        ])\n",
    "\n",
    "        self.block = Block(n_embd = channels, n_head=n_head, dropout = att_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.block(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvMixerBlock(nn.Module):\n",
    "    def __init__(self, kernel_size = 3, channels=16):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.dw_conv = nn.Conv1d(\n",
    "            in_channels = channels, \n",
    "            out_channels = channels, \n",
    "            kernel_size = kernel_size, \n",
    "            stride=1, \n",
    "            padding='same', \n",
    "            dilation=1, \n",
    "            groups=channels,\n",
    "            bias=False, \n",
    "            padding_mode='zeros')\n",
    "\n",
    "        self.pw_conv = nn.Conv1d(\n",
    "            in_channels = channels, \n",
    "            out_channels = channels, \n",
    "            kernel_size = 1, \n",
    "            stride=1, \n",
    "            padding='same', \n",
    "            dilation=1, \n",
    "            groups=1,\n",
    "            bias=False, \n",
    "            padding_mode='zeros')\n",
    "\n",
    "        self.gelu1 = nn.GELU()\n",
    "        self.gelu2 = nn.GELU()       \n",
    "        self.bn1 = nn.BatchNorm1d(channels)             \n",
    "        self.bn2 = nn.BatchNorm1d(channels)     \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "\n",
    "        x = self.dw_conv(x)\n",
    "        x = self.gelu1(x)        \n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = y + x\n",
    "        x = self.pw_conv(x)\n",
    "        x = self.gelu2(x)        \n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16, 60])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvMixerBlock()(torch.ones([8,16,60])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttFeatureExctractor(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size = 3, channels=16, drop_rate = 0.1, n_features=25):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.inputConv = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                out_channels = channels,\n",
    "                                                kernel_size = kernel_size,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(n_features)])\n",
    "        \n",
    "        self.projConv = nn.ModuleList([nn.Conv1d(in_channels = channels,\n",
    "                                                out_channels = channels,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(n_features)])\n",
    "\n",
    "        self.lns = nn.ModuleList([nn.LayerNorm(channels) for _ in range(n_features)])\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 556)\n",
    "        \n",
    "        state_t = x[:, 0:60] # - 273\n",
    "        state_q0001 = x[:, 60:120] #*1_000\n",
    "        state_q0002 = x[:, 120:180] #*1_000\n",
    "        state_q0003 = x[:, 180:240] #*1_000\n",
    "        state_u = x[:, 240:300] #/ 100\n",
    "        state_v = x[:, 300:360] #/ 100\n",
    "    \n",
    "        state_ps = x[:, 360:361] #/ 100_000 - 1\n",
    "        pbuf_SOLIN = x[:, 361:362] #/ 1000\n",
    "        pbuf_LHFLX = x[:, 362:363] #/ 1000\n",
    "        pbuf_SHFLX = x[:, 363:364] #/ 1000\n",
    "        pbuf_TAUX = x[:, 364:365] #/ 1\n",
    "        pbuf_TAUY = x[:, 365:366] #/ 1\n",
    "        pbuf_COSZRS = x[:, 366:367] #/ 1\n",
    "        cam_in_ALDIF = x[:, 367:368] #/ 1\n",
    "        cam_in_ALDIR = x[:, 368:369] #/ 1\n",
    "        cam_in_ASDIF = x[:, 369:370] #/ 1\n",
    "        cam_in_ASDIR = x[:, 370:371] #/ 1\n",
    "        cam_in_LWUP = x[:, 371:372] # / 1000\n",
    "        cam_in_ICEFRAC = x[:, 372:373] #/ 1\n",
    "        cam_in_LANDFRAC = x[:, 373:374] #/1\n",
    "        cam_in_OCNFRAC = x[:, 374:375]  #/1\n",
    "        cam_in_SNOWHLAND = x[:, 375:376]# / 1\n",
    "    \n",
    "        pbuf_ozone = x[:, 376:436] #* 100_000\n",
    "        pbuf_CH4 = x[:, 436:496] #* 100_000\n",
    "        pbuf_N2O = x[:, 496:556] #* 100_000\n",
    "            \n",
    "        inputs_60 = [\n",
    "                state_t,\n",
    "                state_q0001,\n",
    "                state_q0002,\n",
    "                state_q0003, \n",
    "                state_u,\n",
    "                state_v,\n",
    "    \n",
    "                pbuf_ozone,\n",
    "                pbuf_CH4,\n",
    "                pbuf_N2O\n",
    "        ]\n",
    "\n",
    "        inputs_flat = [            \n",
    "                torch.repeat_interleave(state_ps, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SOLIN, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_LHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_TAUX, 60, dim=-1),\n",
    "               torch.repeat_interleave(pbuf_TAUY, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_COSZRS, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIR, 60, dim=-1),\n",
    "               torch.repeat_interleave(cam_in_ASDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ASDIR, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LWUP, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ICEFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LANDFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_OCNFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_SNOWHLAND, 60, dim=-1),\n",
    "        ]\n",
    "\n",
    "        inputs = inputs_60 + inputs_flat\n",
    "        \n",
    "        expanded = []\n",
    "        for i, conv in enumerate(self.inputConv):\n",
    "            t = inputs[i]\n",
    "            t = t.view(-1, 1, 60)\n",
    "            expanded.append(conv(t))\n",
    "\n",
    "        global_input = torch.cat([t.view(-1, self.channels, 60, 1) for t in expanded], axis = -1)\n",
    "        global_input = global_input.mean(axis=-1)\n",
    "        k = global_input#.permute([0,2,1])\n",
    "\n",
    "        var_attention = []\n",
    "        for i, feature in enumerate(expanded):\n",
    "            q = feature.permute([0,2,1]) #B, L, C\n",
    "            v = self.projConv[i](feature).permute([0,2,1]) #B, L, C\n",
    "            \n",
    "            att = nn.Softmax()(torch.matmul(q, k) / self.channels**0.5).permute([0,2,1]) #B, L, L\n",
    "            \n",
    "            y = torch.matmul(att, v) #B, L, L * #B, L, C --> #B, L, C\n",
    "            y = self.lns[i](y)\n",
    "        \n",
    "            var_attention.append((y + q).permute([0,2,1])) #B, C, L\n",
    "        return torch.cat(var_attention, 1)#.permute([0,2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExctractor(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size = 7, channels=16, expand_channels=32, drop_rate = 0.1, n_features=25):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.Scales60 = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                out_channels = channels,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(9)])\n",
    "        \n",
    "        self.ScalesFlat = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                out_channels = channels//2,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(16)])\n",
    "        \n",
    "        self.ConvExt60 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                          for _ in range(9)])\n",
    "        self.ConvExtFlat = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels//2,expand_channels = expand_channels//2, drop_rate=drop_rate)\n",
    "                                          for _ in range(16)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 556)\n",
    "        \n",
    "        state_t = x[:, 0:60] - 273\n",
    "        state_q0001 = x[:, 60:120] *1_000\n",
    "        state_q0002 = x[:, 120:180] *1_000\n",
    "        state_q0003 = x[:, 180:240] *1_000\n",
    "        state_u = x[:, 240:300] / 100\n",
    "        state_v = x[:, 300:360] / 100\n",
    "    \n",
    "        state_ps = x[:, 360:361]/ 100_000 - 1\n",
    "        pbuf_SOLIN = x[:, 361:362] / 1000\n",
    "        pbuf_LHFLX = x[:, 362:363] / 1000\n",
    "        pbuf_SHFLX = x[:, 363:364] / 1000\n",
    "        pbuf_TAUX = x[:, 364:365] / 1\n",
    "        pbuf_TAUY = x[:, 365:366] / 1\n",
    "        pbuf_COSZRS = x[:, 366:367] / 1\n",
    "        cam_in_ALDIF = x[:, 367:368] / 1\n",
    "        cam_in_ALDIR = x[:, 368:369] / 1\n",
    "        cam_in_ASDIF = x[:, 369:370] / 1\n",
    "        cam_in_ASDIR = x[:, 370:371] / 1\n",
    "        cam_in_LWUP = x[:, 371:372] / 1000\n",
    "        cam_in_ICEFRAC = x[:, 372:373] / 1\n",
    "        cam_in_LANDFRAC = x[:, 373:374] /1\n",
    "        cam_in_OCNFRAC = x[:, 374:375]  /1\n",
    "        cam_in_SNOWHLAND = x[:, 375:376] / 1\n",
    "    \n",
    "        pbuf_ozone = x[:, 376:436] * 100_000\n",
    "        pbuf_CH4 = x[:, 436:496] * 100_000\n",
    "        pbuf_N2O = x[:, 496:556] * 100_000\n",
    "            \n",
    "        inputs_60 = [\n",
    "                state_t,\n",
    "                state_q0001,\n",
    "                state_q0002,\n",
    "                state_q0003, \n",
    "                state_u,\n",
    "                state_v,\n",
    "    \n",
    "                pbuf_ozone,\n",
    "                pbuf_CH4,\n",
    "                pbuf_N2O\n",
    "        ]\n",
    "\n",
    "        inputs_flat = [            \n",
    "                torch.repeat_interleave(state_ps, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SOLIN, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_LHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_TAUX, 60, dim=-1),\n",
    "               torch.repeat_interleave(pbuf_TAUY, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_COSZRS, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIR, 60, dim=-1),\n",
    "               torch.repeat_interleave(cam_in_ASDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ASDIR, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LWUP, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ICEFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LANDFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_OCNFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_SNOWHLAND, 60, dim=-1),\n",
    "        ]\n",
    "        \n",
    "        output = []\n",
    "        for i, conv in enumerate(self.ConvExt60):\n",
    "            t = inputs_60[i]\n",
    "            t = t.view(-1, 1, 60)\n",
    "            t = self.Scales60[i](t)\n",
    "            output.append(conv(t))\n",
    "            \n",
    "        for i, conv in enumerate(self.ConvExtFlat):\n",
    "            t = inputs_flat[i]\n",
    "            t = t.view(-1, 1, 60)\n",
    "            t = self.ScalesFlat[i](t)\n",
    "            output.append(conv(t))\n",
    "\n",
    "\n",
    "        return torch.cat(output, 1)#.permute([0,2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 272, 60])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeatureExctractor()(torch.ones([8,556])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEADHead(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = nn.SELU()\n",
    "        self.conv_seq = nn.Conv1d(in_channels = n_embd, out_channels = 6,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same')\n",
    "        \n",
    "        self.conv_flat = nn.Conv1d(in_channels = n_embd, out_channels = 8,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same')\n",
    "\n",
    "        self.expand = nn.Linear(in_features = n_embd, out_features = n_embd*4)\n",
    "        self.out = nn.Linear(in_features = n_embd*4, out_features = 368)\n",
    "        self.drop = nn.Dropout(0.05)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.permute([0,2,1])\n",
    "        \n",
    "        x = self.expand(x)\n",
    "        #x = self.act(x)\n",
    "        x = torch.mean(x, axis = 1)\n",
    "        x = self.drop(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEADHead(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = nn.SELU()\n",
    "        self.conv_seq = nn.Conv1d(in_channels = n_embd, out_channels = 6,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same')\n",
    "        \n",
    "        self.conv_flat = nn.Conv1d(in_channels = n_embd, out_channels = 8,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same')\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = x.permute([0,2,1])\n",
    "        \n",
    "        p_seq = self.conv_seq(x)\n",
    "        p_seq = nn.Flatten()(p_seq)\n",
    "    \n",
    "        p_flat = self.conv_flat(x)\n",
    "        p_flat = torch.mean(p_flat, axis = -1)\n",
    "        \n",
    "        return torch.cat([p_seq, p_flat], axis= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 368])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEADHead(32)(torch.ones([8,32,60])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# block_size = 256\n",
    "# max_iters = 5000\n",
    "# learning_rate = 3e-4\n",
    "# eval_iters = 100\n",
    "# n_embd = 384\n",
    "# n_head = 8\n",
    "# n_layer = 12\n",
    "# dropout = 0.2\n",
    "\n",
    "nn_config = dict(\n",
    "    n_embd = 256,\n",
    "    n_head = 4,\n",
    "    fe_channels = 32, \n",
    "    encoder_layers = 3, \n",
    "    fe_drop_rate = 0.05,\n",
    "    att_drop_rate = 0.1,\n",
    "    n_features = 25,\n",
    "    bottleneck_k_size = 5,\n",
    "    block_kernels = [5, 3]\n",
    ")\n",
    "\n",
    "    \n",
    "class LEADModelAtt(nn.Module):\n",
    "    def __init__(self, n_embd = 64, n_head = 4, encoder_layers = 3, fe_channels=16, fe_drop_rate=0.1, \n",
    "                 att_drop_rate=0.2, n_features = 25, bottleneck_k_size = 3, block_kernels = [5, 3]):\n",
    "        super().__init__()\n",
    "        self.fe = FeatureExctractor(kernel_size = 7, channels=fe_channels, expand_channels=fe_channels*2, drop_rate = fe_drop_rate, n_features=n_features)\n",
    "        self.linearStem = nn.Linear(fe_channels*9 + fe_channels//2 * 16, n_embd)\n",
    "        self.bottleneck = Conv1DBlock(kernel_size=bottleneck_k_size, channels = n_embd, expand_channels = n_embd*2, drop_rate=fe_drop_rate)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[ConvTransBlock(block_kernels = block_kernels, \n",
    "                                                     channels = n_embd, \n",
    "                                                     expand_channels = n_embd*2, \n",
    "                                                     n_head=n_head, \n",
    "                                                     drop_rate = fe_drop_rate, \n",
    "                                                     att_drop_rate = att_drop_rate) for _ in range(encoder_layers)])\n",
    "        \n",
    "        self.head  = LEADHead(n_embd = n_embd)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        #B, T = inputs.shape\n",
    "\n",
    "        xf = self.fe(inputs)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.linearStem(xf)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.bottleneck(xf)\n",
    "        x = xf#.permute([0,2,1])\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# block_size = 256\n",
    "# max_iters = 5000\n",
    "# learning_rate = 3e-4\n",
    "# eval_iters = 100\n",
    "# n_embd = 384\n",
    "# n_head = 8\n",
    "# n_layer = 12\n",
    "# dropout = 0.2\n",
    "\n",
    "nn_config_convmixer = dict(\n",
    "    n_embd = 256,\n",
    "    fe_channels = 64, \n",
    "    fe_drop_rate = 0.05,\n",
    "    att_drop_rate = 0.1,\n",
    "    n_features = 25,\n",
    "    bottleneck_k_size = 5,\n",
    "    convmixer_blocks=6\n",
    ")\n",
    "\n",
    "    \n",
    "class LEAPModelConvmixer(nn.Module):\n",
    "    def __init__(self, n_embd = 64, fe_channels=16, fe_drop_rate=0.1, \n",
    "                 att_drop_rate=0.2, n_features = 25, bottleneck_k_size = 3, convmixer_blocks=3):\n",
    "        super().__init__()\n",
    "        self.fe = FeatureExctractor(kernel_size = 7, channels=fe_channels, expand_channels=fe_channels*2, drop_rate = fe_drop_rate, n_features=n_features)\n",
    "        self.linearStem = nn.Linear(fe_channels*9 + fe_channels//2 * 16, n_embd)\n",
    "        self.bottleneck = Conv1DBlock(kernel_size=bottleneck_k_size, channels = n_embd, expand_channels = n_embd*2, drop_rate=fe_drop_rate)\n",
    "\n",
    "        self.convmixer_blocks = nn.Sequential(*[\n",
    "             ConvMixerBlock(channels=n_embd, kernel_size=3)\n",
    "            for _ in range(convmixer_blocks)\n",
    "        ])\n",
    "\n",
    "        self.head  = LEADHead(n_embd = n_embd)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        #B, T = inputs.shape\n",
    "\n",
    "        xf = self.fe(inputs)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.linearStem(xf)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.bottleneck(xf)\n",
    "        x = xf#.permute([0,2,1])\n",
    "        \n",
    "        x = self.convmixer_blocks(x)\n",
    "        \n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# block_size = 256\n",
    "# max_iters = 5000\n",
    "# learning_rate = 3e-4\n",
    "# eval_iters = 100\n",
    "# n_embd = 384\n",
    "# n_head = 8\n",
    "# n_layer = 12\n",
    "# dropout = 0.2\n",
    "\n",
    "nn_config_sa = dict(\n",
    "    n_embd = 192,\n",
    "    n_head = 4,\n",
    "    fe_channels = 32, \n",
    "    encoder_layers = 2, \n",
    "    fe_drop_rate = 0.1,\n",
    "    att_drop_rate = 0.2,\n",
    "    n_features = 25,\n",
    "    bottleneck_k_size = 3,\n",
    "    block_kernels = [5, 3]\n",
    ")\n",
    "\n",
    "    \n",
    "class LeapModelSelfAtt(nn.Module):\n",
    "    def __init__(self, n_embd = 64, n_head = 4, encoder_layers = 3, fe_channels=16, fe_drop_rate=0.1, \n",
    "                 att_drop_rate=0.2, n_features = 25, bottleneck_k_size = 3, block_kernels = [5, 3]):\n",
    "        super().__init__()\n",
    "        #self.fe = FeatureExctractor(kernel_size = 7, channels=fe_channels, expand_channels=fe_channels*2, drop_rate = fe_drop_rate, n_features=n_features)\n",
    "        self.fe = SelfAttFeatureExctractor(kernel_size = 7, channels=fe_channels, drop_rate = fe_drop_rate, n_features=n_features)\n",
    "        \n",
    "        self.linearStem = nn.Linear(fe_channels*n_features, n_embd)\n",
    "        self.bottleneck = Conv1DBlock(kernel_size=bottleneck_k_size, channels = n_embd, expand_channels = n_embd*2, drop_rate=fe_drop_rate)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[ConvTransBlock(block_kernels = block_kernels, \n",
    "                                                     channels = n_embd, \n",
    "                                                     expand_channels = n_embd*2, \n",
    "                                                     n_head=n_head, \n",
    "                                                     drop_rate = fe_drop_rate, \n",
    "                                                     att_drop_rate = att_drop_rate) for _ in range(encoder_layers)])\n",
    "        \n",
    "        self.head  = LEADHead(n_embd = n_embd)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        #B, T = inputs.shape\n",
    "\n",
    "        xf = self.fe(inputs)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.linearStem(xf)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.bottleneck(xf)\n",
    "        x = xf#.permute([0,2,1])\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 368])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEAPModelConvmixer(**nn_config_convmixer)(torch.ones([8, 556])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# block_size = 256\n",
    "# max_iters = 5000\n",
    "# learning_rate = 3e-4\n",
    "# eval_iters = 100\n",
    "# n_embd = 384\n",
    "# n_head = 8\n",
    "# n_layer = 12\n",
    "# dropout = 0.2\n",
    "\n",
    "nn_config = dict(\n",
    "    n_embd = 256,\n",
    "    n_head = 4,\n",
    "    fe_channels = 64, \n",
    "    encoder_layers = 5, \n",
    "    fe_drop_rate = 0.05,\n",
    "    att_drop_rate = 0.1,\n",
    "    n_features = 25,\n",
    "    bottleneck_k_size = 5,\n",
    "    block_kernels = [5, 3],\n",
    "    out_att_blocks=0\n",
    ")\n",
    "\n",
    "    \n",
    "class LEADModelAtt(nn.Module):\n",
    "    def __init__(self, n_embd = 64, n_head = 4, encoder_layers = 3, fe_channels=16, fe_drop_rate=0.1, \n",
    "                 att_drop_rate=0.2, n_features = 25, bottleneck_k_size = 3, block_kernels = [5, 3], out_att_blocks=3):\n",
    "        super().__init__()\n",
    "        self.fe = FeatureExctractor(kernel_size = 7, channels=fe_channels, expand_channels=fe_channels*2, drop_rate = fe_drop_rate, n_features=n_features)\n",
    "        self.linearStem = nn.Linear(fe_channels*9 + fe_channels//2 * 16, n_embd)\n",
    "        self.bottleneck = Conv1DBlock(kernel_size=bottleneck_k_size, channels = n_embd, expand_channels = n_embd*2, drop_rate=fe_drop_rate)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[ConvTransBlock(block_kernels = block_kernels, \n",
    "                                                     channels = n_embd, \n",
    "                                                     expand_channels = n_embd*2, \n",
    "                                                     n_head=n_head, \n",
    "                                                     drop_rate = fe_drop_rate, \n",
    "                                                     att_drop_rate = att_drop_rate) for _ in range(encoder_layers)])\n",
    "\n",
    "        self.out_att_blocks = nn.Sequential(*[\n",
    "             Block(n_embd = n_embd, n_head=n_head, dropout = att_drop_rate)\n",
    "            for _ in range(out_att_blocks)\n",
    "        ])\n",
    "\n",
    "        \n",
    "        self.head  = LEADHead(n_embd = n_embd)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        #B, T = inputs.shape\n",
    "\n",
    "        xf = self.fe(inputs)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.linearStem(xf)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.bottleneck(xf)\n",
    "        x = xf#.permute([0,2,1])\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "\n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.out_att_blocks(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        \n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_channels = 32\n",
    "fe_channels*9 + fe_channels//2 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 368])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEADModelAtt(**nn_config)(torch.ones([8, 556])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class FocalLossBCE(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha: float = 0.25,\n",
    "            gamma: float = 2,\n",
    "            reduction: str = \"mean\",\n",
    "            bce_weight: float = 1.0,\n",
    "            focal_weight: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction) #, pos_weight=sample_weights_420)\n",
    "        self.bce_weight = bce_weight\n",
    "        self.focal_weight = focal_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "            inputs=logits,\n",
    "            targets=targets,\n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction,\n",
    "        )\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "        return self.bce_weight * bce_loss + self.focal_weight * focall_loss\n",
    "\n",
    "\n",
    "criterion = FocalLossBCE(focal_weight=5, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"train_data_sample.parquet\").sample(100000).drop('sample_id', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = df.iloc[:, 556:].mean().to_numpy()\n",
    "std_y = df.iloc[:, 556:].std().to_numpy()\n",
    "std_y = np.clip(std_y, 1e-10, 1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEAD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, augmentation=False, mode='train'):\n",
    "        if mode == 'train':\n",
    "            self.df = df.reset_index(drop=True)\n",
    "        elif mode == 'valid':\n",
    "            self.df = df.reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.augmentation = augmentation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.df.iloc[idx, :556].to_numpy()\n",
    "        y = self.df.iloc[idx, 556:].to_numpy() \n",
    "        y = (y - mean_y) / std_y\n",
    "        \n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9310e-01,  4.6546e-01,  9.0145e-01,  1.0507e+00,  1.0750e+00,\n",
       "          1.0396e+00,  9.0132e-01,  7.9084e-01,  7.4044e-01,  7.1175e-01,\n",
       "          7.2677e-01,  8.0456e-01,  9.5097e-01,  1.0196e+00,  1.0495e+00,\n",
       "          1.0270e+00,  8.9646e-01,  4.7574e-01,  6.2911e-01, -2.1918e-01,\n",
       "          1.0749e-01, -4.1529e-01, -1.3455e-01, -1.9661e-01, -1.1210e-01,\n",
       "         -3.2775e-02, -1.9219e-03, -5.1749e-02, -6.6888e-02, -7.9437e-02,\n",
       "         -8.0190e-02, -1.0551e-01, -7.3092e-02, -1.2082e-01, -1.8616e-01,\n",
       "         -1.0781e-01, -8.9888e-02, -1.7816e-01, -3.2381e-01, -5.9465e-01,\n",
       "         -1.8779e-01, -3.6965e-01, -5.3235e-01, -8.6318e-01, -2.6964e-01,\n",
       "          2.4821e-01, -1.2117e-01,  1.4580e-02, -1.3717e-01, -4.5144e-01,\n",
       "         -1.2246e+00, -5.9878e-02, -2.0266e-01, -3.7990e-01, -3.5348e-01,\n",
       "         -3.1475e-01, -2.5823e-01, -1.1840e-01,  5.8951e-01,  8.0244e-01,\n",
       "         -4.2185e-06,  1.0310e-06,  4.9206e-07,  1.4455e-07,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00, -6.3854e-05, -2.0777e-04, -2.8183e-04,\n",
       "         -3.2671e-03, -4.9645e-04,  2.6207e-03, -1.0342e-01,  1.9263e-01,\n",
       "         -9.7364e-02,  1.4323e-01,  7.8080e-02,  1.4768e-01,  1.5509e-01,\n",
       "          1.4322e-01,  1.1780e-01,  2.2792e-01,  1.9656e-01,  1.8484e-01,\n",
       "          2.3117e-01,  1.8961e-01,  1.6018e-01,  2.5084e-01,  2.6169e-01,\n",
       "          2.1762e-02,  2.0940e-01,  2.8656e-01,  1.3510e-01,  3.5825e-01,\n",
       "          1.7151e-01,  2.0720e-01,  2.3414e-01,  1.7722e-01, -3.9286e-03,\n",
       "         -4.1676e-02,  7.0357e-03, -1.5929e-02,  6.6786e-02,  7.9721e-02,\n",
       "          4.4023e-02,  6.0618e-02,  1.8589e-01,  3.0492e-01,  3.6841e-01,\n",
       "          4.0140e-01,  4.9521e-01,  4.9477e-01,  7.3184e-01,  4.4323e-01,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  1.4047e-30,  0.0000e+00, -0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  1.6255e-33,  4.4233e-31,\n",
       "          5.6692e-28,  5.2615e-25,  5.7653e-22,  6.4911e-19,  2.5505e-15,\n",
       "          6.2577e-13,  4.0008e-09,  9.0856e-05,  5.4021e-02,  9.1386e-02,\n",
       "          1.1182e-01,  5.0983e-02,  1.7989e-02, -2.9820e-01, -3.6336e-01,\n",
       "         -9.2520e-02, -7.6378e-02, -5.5771e-01, -1.1368e+00, -7.1115e-01,\n",
       "         -4.8716e-01, -4.6233e-01, -1.2429e+00, -5.3922e-01,  2.8671e-01,\n",
       "         -1.8900e-01, -2.1104e-01, -2.6811e-01, -3.9618e-01, -4.6583e-01,\n",
       "          3.7339e-01, -5.3100e-01, -5.7971e-01, -5.5026e-01, -1.0137e+00,\n",
       "         -9.7958e-01, -1.2610e+00, -2.0626e+00, -1.7066e+00, -3.4339e-01,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00, -8.9052e-06, -7.2100e-06, -7.0004e-06,\n",
       "         -9.9042e-04, -2.1762e-04, -7.1208e-04,  4.6562e-03,  4.8575e-03,\n",
       "          8.2514e-03,  1.1344e-02,  1.4218e-02,  1.2907e-02,  1.1174e-02,\n",
       "          2.1447e-02,  3.9354e-02, -3.3395e-02, -3.3090e-02, -2.9277e-02,\n",
       "         -1.7112e-01, -1.5242e-01, -1.3589e-01, -2.5599e-01, -2.9847e-01,\n",
       "         -3.8419e-02, -4.2242e-02, -1.3633e-01, -2.9096e-02,  1.2596e-01,\n",
       "         -6.2554e-03, -2.8258e-02, -4.9882e-03, -1.7084e-02, -6.2857e-03,\n",
       "         -3.9403e-02, -7.3388e-03, -8.8463e-03, -8.9015e-03, -1.5148e-02,\n",
       "         -2.3087e-02, -2.7621e-02, -2.4068e-02, -1.7232e-02, -8.2620e-03,\n",
       "          1.6968e-03,  1.8013e-02,  3.0195e-02,  4.8558e-02,  1.3633e-01,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00, -7.6866e-02,  3.1911e-01, -1.2902e-01,\n",
       "         -3.6721e-03, -8.3010e-03, -5.8686e-02, -8.9161e-02, -7.9207e-02,\n",
       "          4.0888e-02, -9.3115e-02,  2.8271e-02,  4.0617e-02,  8.2180e-02,\n",
       "          1.0597e-01,  1.1941e-01,  1.2704e-01,  1.1670e-01,  1.1665e-01,\n",
       "          1.1044e-01,  9.3929e-02,  7.5005e-02,  6.9347e-02,  5.1734e-02,\n",
       "          3.2881e-02,  5.9538e-02,  2.5575e-02,  3.8011e-02,  4.6319e-02,\n",
       "          3.6499e-02,  8.6313e-02,  8.7723e-02,  8.8229e-02,  1.1783e-01,\n",
       "          1.4484e-01,  2.0657e-01,  1.8209e-01,  1.1817e-01,  4.9173e-02,\n",
       "         -2.4671e-02, -9.2599e-02, -1.7488e-01, -2.0535e-01, -1.7679e-01,\n",
       "         -1.5958e-01, -1.2066e-01,  3.6516e-02,  1.3659e-01, -4.7559e-01,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  1.7971e-01, -5.8011e-02,  2.3604e-03,\n",
       "         -1.1472e-02,  2.5835e-04,  2.8433e-02,  2.0794e-02,  3.0710e-03,\n",
       "         -3.4544e-02, -3.3627e-02,  6.5044e-02, -1.1800e-01, -1.0730e-01,\n",
       "          1.0706e-01, -3.2590e-03,  3.6208e-03,  1.3757e-02,  4.0949e-02,\n",
       "          4.2785e-02,  2.0638e-02, -1.0557e-02, -1.0026e-02, -2.6053e-02,\n",
       "         -2.6355e-02,  2.7330e-02, -1.5845e-02,  6.2775e-03,  3.3580e-03,\n",
       "          1.9487e-04,  3.0485e-02, -1.2872e-02, -7.9581e-02, -1.0197e-01,\n",
       "         -9.1883e-02, -1.0028e-02, -3.5694e-02, -5.9604e-02, -4.7079e-02,\n",
       "         -2.8432e-02,  3.9695e-02,  5.5593e-02,  5.8700e-02,  6.6256e-02,\n",
       "          1.0036e-01,  5.8138e-02,  4.6556e-02,  4.8922e-01, -3.5631e-01,\n",
       "          6.0510e-01,  1.6869e-01, -3.6188e-01, -3.5183e-01, -6.3486e-03,\n",
       "         -1.3716e-01,  1.9912e+00,  2.5398e+00]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEAD_Dataset(df).__getitem__(3)[1].view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4763e-01, -1.7441e-01, -1.2951e-01, -1.2872e-01, -4.8616e-02,\n",
       "         -1.2692e-01, -1.5802e-01, -7.1071e-02,  8.4509e-03, -1.3914e-01,\n",
       "         -8.4053e-02, -1.0286e-01, -1.2038e-01, -9.4866e-02, -9.6882e-02,\n",
       "         -1.8872e-01, -9.1128e-02, -1.4886e-02, -1.8602e-01, -4.8317e-02,\n",
       "         -9.0935e-02, -1.6477e-01, -2.9389e-02,  1.8322e-02, -7.8579e-02,\n",
       "         -1.2545e-01, -8.8488e-02, -1.0550e-01, -9.1428e-02, -8.4059e-02,\n",
       "         -1.6925e-01, -2.3096e-01, -1.3539e-01, -1.3797e-01, -6.7603e-02,\n",
       "         -7.2184e-02, -1.5089e-01, -9.3572e-03, -5.7932e-02, -2.0231e-01,\n",
       "         -6.4616e-02, -2.0541e-01, -2.2879e-01, -3.0158e-01, -3.5694e-01,\n",
       "         -2.9445e-01, -3.2000e-01, -3.5246e-01, -3.5334e-01, -3.5246e-01,\n",
       "         -3.7985e-01, -4.8834e-01, -4.6142e-01, -4.1793e-01, -4.0620e-01,\n",
       "         -4.2311e-01, -3.9149e-01, -3.5878e-01, -3.2743e-01, -3.0547e-01,\n",
       "         -9.2148e-02, -6.9875e-02, -2.4380e-01, -1.7994e-01, -2.7905e-01,\n",
       "         -1.8783e-01, -1.5013e-01, -8.7869e-03, -1.2805e-01, -1.3086e-01,\n",
       "         -4.5113e-02, -1.1759e-01, -2.0573e-01, -2.2277e-01, -1.6648e-01,\n",
       "         -1.7492e-01, -1.2726e-01, -2.1240e-01, -1.7006e-01, -1.1513e-01,\n",
       "         -2.9312e-01, -3.0498e-01, -1.9469e-01, -1.3759e-01, -2.4068e-01,\n",
       "         -1.3263e-01, -6.7522e-02, -6.4400e-02, -2.3469e-01, -7.0728e-02,\n",
       "         -6.3791e-02, -1.1055e-01, -1.3183e-01, -1.7160e-01, -8.0962e-02,\n",
       "         -1.5577e-01,  1.8187e-01,  1.1070e-01,  3.4548e-01,  3.7744e-01,\n",
       "          3.6622e-01,  4.6254e-01,  3.8020e-01,  3.8946e-01,  3.1173e-01,\n",
       "          3.5853e-01,  2.9513e-01,  4.8380e-01,  4.3855e-01,  3.6931e-01,\n",
       "          3.9586e-01,  3.5739e-01,  4.1531e-01,  3.5916e-01,  4.5976e-01,\n",
       "          3.8362e-01,  4.8574e-01,  3.2801e-01,  4.4768e-01,  4.5649e-01,\n",
       "         -3.3762e-01, -3.8990e-01, -3.7533e-01, -4.0759e-01, -3.8332e-01,\n",
       "         -3.8301e-01, -4.0166e-01, -3.9359e-01, -3.9582e-01, -3.3499e-01,\n",
       "         -3.7438e-01, -2.6110e-01, -3.3144e-01, -3.7330e-01, -2.6534e-01,\n",
       "         -3.4829e-01, -3.5804e-01, -4.4252e-01, -3.7431e-01, -3.4235e-01,\n",
       "         -3.8335e-01, -3.5904e-01, -3.2167e-01, -2.8948e-01, -3.4925e-01,\n",
       "         -3.3244e-01, -2.9900e-01, -2.9316e-01, -3.5551e-01, -3.1110e-01,\n",
       "         -4.0261e-01, -4.1370e-01, -3.5806e-01, -4.2155e-01, -5.0269e-01,\n",
       "         -5.4675e-01, -5.6641e-01, -6.0134e-01, -6.3715e-01, -7.4973e-01,\n",
       "         -8.7284e-01, -9.2310e-01, -7.9126e-01, -7.5119e-01, -7.1428e-01,\n",
       "         -7.5318e-01, -8.4204e-01, -7.4948e-01, -6.8133e-01, -7.7669e-01,\n",
       "         -8.6527e-01, -7.4232e-01, -7.1023e-01, -6.6561e-01, -6.7376e-01,\n",
       "         -6.4778e-01, -6.2361e-01, -7.9226e-01, -5.9016e-01, -6.1969e-01,\n",
       "         -5.6598e-01, -5.7348e-01, -6.6682e-01, -5.5157e-01, -5.3531e-01,\n",
       "         -5.6090e-01, -5.9688e-01, -6.2946e-01, -5.7224e-01, -7.0781e-01,\n",
       "         -5.1715e-01, -6.6076e-01, -5.5943e-01, -6.0967e-01, -6.3585e-01,\n",
       "         -6.3140e-01, -5.4710e-01, -4.3080e-01, -4.6232e-01, -5.4469e-01,\n",
       "         -6.0628e-01, -6.8954e-01, -7.0162e-01, -6.5282e-01, -6.7687e-01,\n",
       "         -6.8777e-01, -6.0349e-01, -6.4652e-01, -6.5578e-01, -5.6332e-01,\n",
       "         -5.9849e-01, -5.5266e-01, -6.4333e-01, -5.9094e-01, -6.7344e-01,\n",
       "         -6.0649e-01, -5.3438e-01, -5.9355e-01, -5.1681e-01, -4.3690e-01,\n",
       "         -1.5814e-01, -3.4461e-02,  3.4972e-02,  7.1192e-02,  5.8305e-02,\n",
       "          1.2619e-01,  1.5669e-01,  7.9422e-02,  1.1044e-01,  1.6819e-01,\n",
       "          1.6046e-01,  9.1603e-02,  2.1389e-01,  1.7351e-01,  6.8592e-03,\n",
       "          1.7625e-01,  1.9130e-01,  1.1293e-01,  1.6764e-01,  2.7130e-01,\n",
       "         -6.6764e-02,  2.7508e-02, -9.9280e-02, -8.7692e-02,  3.0292e-02,\n",
       "          7.3000e-02,  6.8045e-02, -5.8094e-02,  1.7715e-03, -1.3746e-01,\n",
       "         -2.0511e-01,  2.6452e-02, -1.0602e-01, -1.4838e-01, -2.1201e-01,\n",
       "         -1.9395e-01, -1.4860e-01, -2.5230e-01, -2.6867e-01,  4.6985e-03,\n",
       "         -1.7707e-01, -7.8432e-02, -2.4417e-01, -1.2082e-01, -1.6707e-01,\n",
       "         -1.4202e-01, -2.2879e-02, -1.6494e-01, -9.4005e-04, -7.7150e-02,\n",
       "         -1.1179e-01, -6.0280e-02,  4.4676e-02, -2.2994e-02, -5.7103e-02,\n",
       "         -6.4468e-03, -1.3007e-01, -4.6969e-02, -1.2059e-01, -1.2505e-01,\n",
       "         -1.2759e-01,  3.2104e-02, -6.4422e-02, -2.5579e-02, -1.6162e-01,\n",
       "         -4.1388e-02, -1.0916e-01,  5.7976e-02, -1.9219e-02, -3.2281e-02,\n",
       "          1.0537e-02, -2.4225e-02, -1.0032e-01,  5.9678e-02, -1.8941e-02,\n",
       "         -1.0344e-01, -6.3789e-02, -8.3697e-02, -2.4224e-03,  1.2153e-01,\n",
       "          6.3619e-01,  6.7652e-01,  7.3095e-01,  6.8155e-01,  7.4378e-01,\n",
       "          9.8282e-01,  8.6737e-01,  6.1313e-01,  5.8801e-01,  5.7922e-01,\n",
       "          6.8123e-01,  6.5751e-01,  5.5510e-01,  5.8179e-01,  5.2791e-01,\n",
       "          5.3814e-01,  6.3306e-01,  5.4358e-01,  6.0413e-01,  6.6798e-01,\n",
       "          5.8718e-01,  5.6546e-01,  5.7295e-01,  6.4280e-01,  6.3814e-01,\n",
       "          5.7711e-01,  6.2655e-01,  6.4954e-01,  6.1657e-01,  6.9076e-01,\n",
       "          6.5097e-01,  6.2087e-01,  7.5430e-01,  7.5556e-01,  7.9770e-01,\n",
       "          8.5628e-01,  8.0922e-01,  7.9885e-01,  8.5055e-01,  5.8850e-01,\n",
       "          4.0441e-01,  2.5248e-01,  2.0855e-01,  1.4413e-01,  3.9142e-02,\n",
       "          4.3800e-02,  8.9209e-02, -2.5894e-03,  1.2566e-02,  1.4019e-01,\n",
       "          6.8477e-02, -7.4997e-02,  4.0125e-02, -8.3737e-02, -1.1893e-01,\n",
       "         -1.8807e-01, -8.2939e-02, -1.4456e-01, -2.9976e-02, -2.8930e-02,\n",
       "          5.2532e-02, -3.1529e-01,  2.0269e-01, -3.1093e-01,  1.1019e-01,\n",
       "         -7.2576e-02,  7.0383e-02, -3.4113e-02],\n",
       "        [-1.3540e-01, -8.5305e-02, -8.1872e-02, -1.7092e-01, -2.9074e-02,\n",
       "          1.5018e-01, -1.0453e-01, -3.7010e-02, -1.1886e-01, -8.3949e-02,\n",
       "         -1.2840e-01, -1.2588e-01, -8.8525e-02, -1.2657e-01, -1.6563e-01,\n",
       "         -1.1365e-01, -5.0581e-03, -3.0820e-02, -6.6266e-02, -6.4279e-02,\n",
       "         -1.1760e-01, -8.7250e-02, -5.9305e-02, -5.3944e-02, -2.2887e-02,\n",
       "         -6.9279e-02, -3.5145e-02, -5.3618e-02, -8.9640e-02, -8.7294e-02,\n",
       "         -9.2151e-02, -1.3037e-02, -3.8947e-02, -1.1910e-02,  8.5057e-04,\n",
       "          3.0235e-02, -5.4096e-02,  2.3951e-02, -3.8581e-02,  7.7535e-02,\n",
       "         -9.8772e-03, -3.2405e-02,  1.1276e-02, -1.6313e-01, -7.6113e-02,\n",
       "         -1.9466e-01, -3.3644e-01, -2.0984e-01, -2.5062e-01, -2.6404e-01,\n",
       "         -3.2943e-01, -3.0764e-01, -4.0700e-01, -3.5172e-01, -4.0492e-01,\n",
       "         -3.6909e-01, -3.4224e-01, -3.3260e-01, -4.0009e-01, -2.8139e-01,\n",
       "         -2.0772e-01, -1.8143e-01, -1.4481e-01, -1.5771e-01, -1.6200e-01,\n",
       "         -7.6505e-02, -1.4497e-01, -1.8265e-01, -6.1566e-02, -1.9810e-01,\n",
       "         -2.0624e-01, -2.2451e-01, -2.6217e-01, -1.3768e-01, -1.6208e-01,\n",
       "         -5.9590e-03, -4.0593e-02, -1.3344e-01, -1.6208e-01, -2.6914e-01,\n",
       "         -1.8871e-01, -1.7963e-01, -1.2321e-01, -1.3765e-01, -2.0414e-01,\n",
       "         -1.1830e-01, -3.1066e-01, -8.3911e-02, -1.9655e-01, -1.7013e-01,\n",
       "         -2.0331e-01, -1.2470e-01, -2.0448e-01, -1.5481e-01, -1.8570e-01,\n",
       "         -1.4307e-01, -7.3427e-02, -7.1239e-02, -3.3450e-02,  2.7616e-02,\n",
       "         -8.4091e-03,  1.0632e-01,  8.7601e-02,  1.5433e-01,  1.2720e-01,\n",
       "          2.0723e-01,  2.7103e-01,  3.6419e-01,  2.7969e-01,  3.0939e-01,\n",
       "          1.1777e-01,  3.8754e-01,  3.2271e-01,  3.2163e-01,  3.9057e-01,\n",
       "          3.5560e-01,  4.3517e-01,  3.1984e-01,  3.0889e-01,  3.7479e-01,\n",
       "         -3.1454e-01, -3.9028e-01, -3.2330e-01, -3.0765e-01, -4.4673e-01,\n",
       "         -4.1536e-01, -3.0124e-01, -4.5598e-01, -3.8942e-01, -4.0917e-01,\n",
       "         -4.4545e-01, -3.3686e-01, -3.9332e-01, -4.4850e-01, -3.6447e-01,\n",
       "         -3.0143e-01, -4.0044e-01, -3.0624e-01, -3.2206e-01, -3.7956e-01,\n",
       "         -3.6605e-01, -2.2628e-01, -3.2148e-01, -3.2820e-01, -3.2510e-01,\n",
       "         -3.9028e-01, -3.6732e-01, -3.4453e-01, -4.3924e-01, -3.4150e-01,\n",
       "         -2.6683e-01, -4.0364e-01, -4.1546e-01, -3.8484e-01, -4.1499e-01,\n",
       "         -4.7614e-01, -5.6425e-01, -5.7936e-01, -7.0208e-01, -6.8750e-01,\n",
       "         -8.8798e-01, -8.3591e-01, -1.0503e+00, -9.3010e-01, -8.5504e-01,\n",
       "         -8.7267e-01, -8.8664e-01, -8.8415e-01, -7.8417e-01, -8.2599e-01,\n",
       "         -8.1497e-01, -7.7881e-01, -6.4228e-01, -7.2560e-01, -6.3049e-01,\n",
       "         -7.5331e-01, -6.2208e-01, -5.5484e-01, -5.9237e-01, -5.4001e-01,\n",
       "         -4.8210e-01, -5.3873e-01, -4.9081e-01, -6.0142e-01, -5.6279e-01,\n",
       "         -4.7651e-01, -5.8305e-01, -5.6466e-01, -6.8570e-01, -5.4618e-01,\n",
       "         -5.7540e-01, -6.1219e-01, -6.5249e-01, -4.8898e-01, -6.5606e-01,\n",
       "         -5.7313e-01, -6.1713e-01, -6.5760e-01, -5.6362e-01, -6.0250e-01,\n",
       "         -5.7656e-01, -6.8145e-01, -6.0663e-01, -5.4963e-01, -5.9431e-01,\n",
       "         -5.9967e-01, -6.1417e-01, -5.0283e-01, -6.1441e-01, -5.7132e-01,\n",
       "         -6.1449e-01, -6.7042e-01, -6.0692e-01, -5.8300e-01, -6.3210e-01,\n",
       "         -5.2859e-01, -5.0961e-01, -4.8482e-01, -3.9510e-01, -3.6639e-01,\n",
       "         -2.7253e-01, -1.6283e-01,  1.5512e-02,  4.7672e-02,  4.7153e-02,\n",
       "          1.1038e-02,  1.4833e-01,  3.9648e-02,  2.0215e-01,  1.5127e-01,\n",
       "          2.6225e-01,  2.4154e-01,  2.0483e-01,  1.6675e-01,  1.7642e-01,\n",
       "          2.3435e-01,  3.5838e-01,  2.2253e-01,  2.7730e-01,  2.2274e-01,\n",
       "         -1.2889e-01, -1.9580e-01, -1.3112e-01, -1.4145e-01, -1.5302e-01,\n",
       "         -8.9568e-02, -1.5449e-01, -2.3851e-01, -1.6709e-01, -2.1551e-01,\n",
       "         -9.3230e-02, -7.2820e-02, -1.6248e-01, -1.5286e-01, -2.1712e-01,\n",
       "         -1.1310e-01, -1.6087e-01, -1.9821e-01, -1.5385e-01, -9.6929e-02,\n",
       "         -3.5374e-02, -1.4282e-01, -2.2387e-01, -6.6360e-02, -1.1947e-01,\n",
       "         -1.2539e-01, -1.1535e-01, -7.0535e-02, -1.9486e-01, -1.6430e-01,\n",
       "         -1.5219e-01, -1.4554e-01, -1.0948e-01, -1.2970e-01, -1.4535e-01,\n",
       "         -1.2727e-01, -9.6351e-02, -2.4566e-01, -1.3305e-01, -8.0123e-02,\n",
       "         -3.3468e-02, -2.0565e-01, -3.9712e-02, -1.3915e-01, -9.6753e-02,\n",
       "         -1.7069e-01,  4.3148e-02, -1.4819e-01, -1.5038e-01,  3.1577e-02,\n",
       "         -8.8727e-02, -5.8267e-02,  8.5894e-03,  5.4797e-02, -8.4836e-02,\n",
       "         -4.3934e-02,  4.6532e-02, -1.6003e-02,  4.6574e-02,  7.7686e-02,\n",
       "          4.9602e-01,  6.2607e-01,  5.6749e-01,  7.9254e-01,  7.4933e-01,\n",
       "          5.4928e-01,  4.0206e-01,  5.7668e-01,  5.1841e-01,  5.2903e-01,\n",
       "          5.4627e-01,  5.9296e-01,  6.2788e-01,  4.4315e-01,  5.0970e-01,\n",
       "          6.2874e-01,  5.1182e-01,  5.6772e-01,  5.2876e-01,  6.4969e-01,\n",
       "          4.8772e-01,  5.4275e-01,  4.7529e-01,  5.8723e-01,  5.7529e-01,\n",
       "          6.8875e-01,  6.9943e-01,  6.6387e-01,  6.4604e-01,  6.5616e-01,\n",
       "          5.6748e-01,  6.7619e-01,  6.7727e-01,  5.8066e-01,  6.3749e-01,\n",
       "          6.6457e-01,  6.9823e-01,  6.2909e-01,  6.1391e-01,  5.6034e-01,\n",
       "          3.9742e-01,  3.0432e-01,  1.9065e-01,  2.0909e-01,  3.7008e-02,\n",
       "         -1.2292e-01, -1.0503e-01, -1.0129e-01, -8.6518e-02, -1.8606e-02,\n",
       "         -6.2293e-02, -1.2334e-01, -4.7525e-02, -2.3026e-02, -1.1011e-01,\n",
       "         -2.0125e-02, -6.0765e-03, -1.4054e-01, -1.5186e-01, -9.6286e-02,\n",
       "          4.5196e-02, -3.5891e-01,  7.9422e-02, -2.5157e-01, -2.4248e-02,\n",
       "         -2.2405e-01,  2.5387e-02, -5.9989e-02]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEADModelAtt(**nn_config)(torch.cat([LEAD_Dataset(df).__getitem__(3)[0].view(1, -1), LEAD_Dataset(df).__getitem__(5)[0].view(1, -1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LEAD_Dataset(df).__getitem__(63)[1]\n",
    "b = LEAD_Dataset(df).__getitem__(9)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([368])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2001)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss()(torch.tensor(np.expand_dims(a, 0)), torch.tensor(np.expand_dims(b, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_pred:torch.Tensor, y_true:torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the R^2 (coefficient of determination) regression score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : torch.Tensor\n",
    "        The predicted values.\n",
    "    y_true : torch.Tensor\n",
    "        The true values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The R^2 score, a float value.\n",
    "    \"\"\"\n",
    "    \n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    \n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    \n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "#from torcheval.metrics import R2Score \n",
    "from torchmetrics.regression import R2Score\n",
    "metric = R2Score()\n",
    "\n",
    "\n",
    "\n",
    "class LEADModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        #self.backbone = LEADModelAtt(**nn_config).to(config.DEVICE)\n",
    "        #self.backbone = LEAPModelConvmixer(**nn_config_convmixer).to(config.DEVICE)\n",
    "        self.backbone = LeapModelSelfAtt(**nn_config_sa).to(config.DEVICE)\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.metric = R2Score()\n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=config.EPOCHS,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-7,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device).float()\n",
    "        target = target.to(self.device).float()\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device).float()\n",
    "        target = target.to(self.device).float()\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        print('\\n')\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint: dict) -> None:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        model_state_dict = self.state_dict()\n",
    "        is_changed = False\n",
    "        for k in state_dict:\n",
    "            if k in model_state_dict:\n",
    "                if state_dict[k].shape != model_state_dict[k].shape:\n",
    "                    print(f\"Skip loading parameter: {k}, \"\n",
    "                                f\"required shape: {model_state_dict[k].shape}, \"\n",
    "                                f\"loaded shape: {state_dict[k].shape}\")\n",
    "                    state_dict[k] = model_state_dict[k]\n",
    "                    is_changed = True\n",
    "            else:\n",
    "                print(f\"Dropping parameter {k}\")\n",
    "                is_changed = True\n",
    "\n",
    "        if is_changed:\n",
    "            checkpoint.pop(\"optimizer_states\", None)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        #output_val = nn.Sigmoid()(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        #output_val = torch.cat([x['logits'] for x in outputs], dim=0).cpu().detach()\n",
    "        #target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        output_val = torch.cat([x['logits'] for x in outputs], dim=0)#.cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0)#.cpu().detach()\n",
    "        \n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        # == record ==\n",
    "        print(f\"val_loss: {val_loss}\")\n",
    "        self.log('val_loss', val_loss, True)\n",
    "        \n",
    "        val_loss = val_loss.cpu().detach()\n",
    "\n",
    "    \n",
    "        #output_val = nn.Sigmoid()(output_val).cpu().detach()\n",
    "        output_val = output_val.cpu().detach()\n",
    "        target_val = target_val.cpu().detach()\n",
    "\n",
    "  \n",
    "        y = (output_val * std_y) + mean_y\n",
    "        \n",
    "        y_pred = target_val\n",
    "        y_pred[:, std_y < 1e-9] = 0\n",
    "        y_pred = (y_pred * std_y) + mean_y\n",
    "\n",
    "        # r2=0\n",
    "        # for i in range(368):\n",
    "        #     r2_i = self.metric(y_pred[:, i], y[:, i])\n",
    "        #     r2 += r2_i\n",
    "        # val_score  = r2/ 368\n",
    "        #val_score = self.metric(y_pred, y)\n",
    "\n",
    "        r2=0\n",
    "        for i in range(368):\n",
    "            r2_i = self.metric(output_val[:, i], target_val[:, i])\n",
    "            if r2_i > 1e-6:\n",
    "                r2 += r2_i\n",
    "        val_score  = r2/ 368\n",
    "\n",
    "        \n",
    "        \n",
    "        # self.metric.update(target_val, output_val)\n",
    "        # val_score = self.metric.compute()\n",
    "        \n",
    "        # target to one-hot\n",
    "        #target_val = torch.nn.functional.one_hot(target_val, len(label_list))\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        # gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        # pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        # gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        # pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        # val_score = score(gt_df.drop(cols_drop_on_val, axis=1), pred_df.drop(cols_drop_on_val, axis=1), row_id_column_name='id')\n",
    "        \n",
    "        print(f\"val_R2: {val_score}\")\n",
    "        \n",
    "        self.log(\"val_R2\", val_score, True)\n",
    "        \n",
    "        # clear validation outputs\n",
    "        self.validation_step_outputs = list()\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_R2': val_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CHECKPOINT = False\n",
    "#CHK_PATH = './pretrain_checkpoints/eca_nfnet_l0_fold_0_0.97126.ckpt'\n",
    "\n",
    "\n",
    "def run_training(fold_id, total_df):\n",
    "    print('================================================================')\n",
    "    print(f\"==== Running training for fold {fold_id} ====\")\n",
    "    \n",
    "    # == create dataset and dataloader ==\n",
    "    train_df = total_df[total_df['fold'] != fold_id].drop('fold', axis=1).copy()\n",
    "    valid_df = total_df[total_df['fold'] == fold_id].drop('fold', axis=1).copy()\n",
    "    \n",
    "    print(f'Train Samples: {len(train_df)}')\n",
    "    print(f'Valid Samples: {len(valid_df)}')\n",
    "    \n",
    "  \n",
    "    train_ds = LEAD_Dataset(train_df)\n",
    "    val_ds =  LEAD_Dataset(valid_df)\n",
    "    #val_ds = WaveAllFileDataset(df=valid_df, name_col=\"filepath\", **val_dataset_config)\n",
    "    \n",
    "    \n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        #num_workers=config.N_WORKERS,\n",
    "        pin_memory=True,\n",
    "        #persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=config.BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        #num_workers=config.N_WORKERS,\n",
    "        pin_memory=True,\n",
    "        #persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # == init model ==\n",
    "    if USE_CHECKPOINT:\n",
    "        model = LEADModel.load_from_checkpoint(CHK_PATH, strict=False)\n",
    "    else:\n",
    "        model = LEADModel()\n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                          dirpath=config.OUTPUT_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=True,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='min')\n",
    "\n",
    "    callbacks_to_use = [checkpoint_callback, TQDMProgressBar(refresh_rate=1)]\n",
    "\n",
    "    print(f'trainer')\n",
    "    # == init trainer ==\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.EPOCHS,\n",
    "        val_check_interval=1.,\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=callbacks_to_use,\n",
    "        enable_model_summary=False,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else 'auto',\n",
    "        deterministic=True,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "    )\n",
    "    \n",
    "    # == Training ==\n",
    "    trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    # == Prediction ==\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    weights = torch.load(best_model_path)['state_dict']\n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.target<30].reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=config.FOLDS, shuffle=True, random_state=config.SEED)\n",
    "df['fold'] = 0\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.EPOCHS = 10\n",
    "#config.LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "def disable_logging_during_tests():\n",
    "    # Store the current log level to restore it later\n",
    "    original_log_level = logging.getLogger().getEffectiveLevel()\n",
    "\n",
    "    # Set the log level to a higher level, e.g., WARNING or CRITICAL\n",
    "    logging.disable(logging.ERROR)\n",
    "\n",
    "    # Run your tests here\n",
    "\n",
    "    # Restore the original log level after the tests\n",
    "    logging.disable(original_log_level)\n",
    "\n",
    "# Call this function before running your tests\n",
    "disable_logging_during_tests()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "==== Running training for fold 0 ====\n",
      "Train Samples: 85714\n",
      "Valid Samples: 14286\n",
      "trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory E:\\PycharmProjects\\LEAP\\output exists and is not empty.\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf9a4cf030b4d859dbb198ca7cf082f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7267396450042725\n",
      "val_R2: 0.04140517860651016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8799475431442261\n",
      "val_R2: 0.0091019868850708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8439425826072693\n",
      "val_R2: 0.01818281225860119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7283050417900085\n",
      "val_R2: 0.040166694670915604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.811728298664093\n",
      "val_R2: 0.033796437084674835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.8380553722381592\n",
      "val_R2: 0.008417516946792603\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7335361242294312\n",
      "val_R2: 0.03158706799149513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7348616123199463\n",
      "val_R2: 0.03480054438114166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7427145838737488\n",
      "val_R2: 0.022251347079873085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7218511700630188\n",
      "val_R2: 0.040942803025245667\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7699981927871704\n",
      "val_R2: 0.0053141978569328785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7190405130386353\n",
      "val_R2: 0.04740745201706886\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7888358235359192\n",
      "val_R2: 0.00562881538644433\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7428426742553711\n",
      "val_R2: 0.020871788263320923\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa604597a8040b9a779f9aae709b063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.7834185361862183\n"
     ]
    }
   ],
   "source": [
    "selected_folds = [0,4,5]\n",
    "    \n",
    "# training\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "\n",
    "for f in range(config.FOLDS):\n",
    "    \n",
    "    if f not in selected_folds:\n",
    "        continue\n",
    "    \n",
    "    # get validation index\n",
    "    #val_idx = list(train_df[train_df['fold'] == f].index)\n",
    "    \n",
    "    # main loop of f-fold\n",
    "    trainer = run_training(f, df)\n",
    "    \n",
    "\n",
    "    \n",
    "    # only training one fold\n",
    "    #break\n",
    "\n",
    "\n",
    "# for idx, val_score in enumerate(fold_val_score_list):\n",
    "#     print(f'Fold {idx} Val Score: {val_score:.5f}')\n",
    "\n",
    "# oof_gt_df = oof_df[['samplename'] + label_list].copy()\n",
    "# oof_pred_df = oof_df[['samplename'] + pred_cols].copy()\n",
    "# oof_pred_df.columns = ['samplename'] + label_list\n",
    "# oof_score = score(oof_gt_df, oof_pred_df, 'samplename')\n",
    "# print(f'OOF Score: {oof_score:.5f}')\n",
    "\n",
    "#oof_df.to_csv(f\"{config.OUTPUT_DIR}/oof_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(tokenized, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEADModelAtt(**nn_config).to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "        LEAD_Dataset(df.drop('fold', axis=1)),\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        #persistent_workers=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 2000\n",
    "eval_iters = 1000\n",
    "learning_rate = 3e-4\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iters):\n",
    "    #print(iter)\n",
    "    # if iter % eval_iters == 0:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    #xb, yb = LEAD_Dataset(df.drop('fold', axis=1)).__getitem__(iter)\n",
    "\n",
    "    xb, yb = next(iter(train_dl))\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits = model.forward(xb.to('cuda').float(), yb.to('cuda').float())\n",
    "\n",
    "    loss = nn.MSELoss()(logits, yb.to('cuda').float())\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.EPOCHS = 25  # max epochs\n",
    "#config.LR = 3e-4  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/version_0/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_metric_learning.losses import ArcFaceLoss\n",
    "num_classes = 384\n",
    "embedding_size = 64\n",
    "\n",
    "loss_func = ArcFaceLoss(num_classes, embedding_size, margin=28.6, scale=64).to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.range(1, 100)#.view(-1,1)\n",
    "emb = torch.rand([100, 64])\n",
    "\n",
    "la  = loss_func(emb, label.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-metric-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2091745,
     "sourceId": 25954,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 3221581,
     "sourceId": 33246,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 5188730,
     "sourceId": 44224,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 1292430,
     "sourceId": 19596,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 8508280,
     "datasetId": 4979005,
     "sourceId": 8374380,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8528074,
     "datasetId": 4990143,
     "sourceId": 8393186,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 5267533,
     "datasetId": 3020983,
     "sourceId": 5195317,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8207598,
     "datasetId": 4776799,
     "sourceId": 8090934,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8496617,
     "datasetId": 4970788,
     "sourceId": 8363310,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 152757,
     "datasetId": 3151,
     "sourceId": 142598,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 3680898,
     "datasetId": 2172852,
     "sourceId": 3627245,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 1520932,
     "datasetId": 726237,
     "sourceId": 1487019,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 1521030,
     "datasetId": 726312,
     "sourceId": 1487116,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 154204277,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 167220511,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
