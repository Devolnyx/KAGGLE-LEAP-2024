{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "\n",
    "Import all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import librosa\n",
    "from scipy import signal as sci_signal\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "import albumentations as albu\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "\n",
    "# import score function of BirdCLEF\n",
    "#sys.path.append('/kaggle/input/birdclef-roc-auc')\n",
    "#sys.path.append('/kaggle/usr/lib/kaggle_metric_utilities')\n",
    "#from metric import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vyacheslav\\AppData\\Local\\Temp\\ipykernel_2232\\1764714236.py:3: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = mpl.cm.get_cmap('coolwarm')\n"
     ]
    }
   ],
   "source": [
    "# Import for visualization\n",
    "import matplotlib as mpl\n",
    "cmap = mpl.cm.get_cmap('coolwarm')\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display as lid\n",
    "import IPython.display as ipd\n",
    "#import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "Hyper-paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 28082015\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    \n",
    "    # == global config ==\n",
    "    SEED = 28082015  # random seed\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # device to be used\n",
    "    MIXED_PRECISION = False  # whether to use mixed-16 precision\n",
    "    OUTPUT_DIR = './output/'  # output folder\n",
    "    \n",
    "    # == data config ==\n",
    "    DATA_ROOT = 'E:/PycharmProjects/birdclef24/data'  # root folder\n",
    "    PREPROCESSED_DATA_ROOT = '/kaggle/input/birdclef24-spectrograms-via-cupy'\n",
    "    LOAD_DATA = True  # whether to load data from pre-processed dataset\n",
    "\n",
    "    \n",
    "    # == model config ==\n",
    "    MODEL_TYPE = 'efficientnet_b0'  # model type\n",
    "    \n",
    "    # == dataset config ==\n",
    "    BATCH_SIZE = 256  # batch size of each step\n",
    "    N_WORKERS = 6  # number of workers\n",
    "    \n",
    "    \n",
    "    # == training config ==\n",
    "    FOLDS = 7  # n fold\n",
    "    EPOCHS = 200  # max epochs\n",
    "    LR = 7e-4  # learning rate\n",
    "    WEIGHT_DECAY = 9e-6  # weight decay of optimizer\n",
    "    \n",
    "    # == other config ==\n",
    "    VISUALIZE = True  # whether to visualize data and batch\n",
    "    \n",
    "    \n",
    "print('fix seed')\n",
    "pl.seed_everything(config.SEED, workers=True)\n",
    "\n",
    "CFG = config\n",
    "\n",
    "TARGET_WEIGHTS = [30981.265271661872, 22502.432413914863, 18894.14713004499, 14514.244730542465, 10944.348069459196, 9065.01072024503, 9663.669038687454, 12688.557362943708, 19890.17226527665, 25831.37317235381, 33890.367561807274, 44122.94111025334, 59811.25595068309, 79434.07500078829, 107358.80916894016, 135720.8418348218, 149399.8411114814, 128492.95185325432, 91746.23687305572, 72748.76911097553, 66531.53596840335, 62932.30598423903, 56610.26874314136, 49473.14369220607, 43029.18495420936, 36912.67491908133, 31486.93117928144, 26898.072997215502, 23316.638282978325, 20459.73133196152, 18385.68309639014, 17111.405107656312, 16337.80991958771, 15857.759882318944, 15580.902485189716, 15497.59045982052, 15612.2556996736, 15797.88455410361, 15974.218740897895, 16130.395527176632, 16261.310866446129, 16371.892401608216, 16397.019695140876, 16325.463899570548, 16228.641108112768, 16191.809643436269, 16341.207925934068, 16645.711351490587, 17005.493716683693, 17430.29874509864, 17907.24023203076, 18431.55334008694, 19032.471309392287, 19701.355113141435, 20408.236605392685, 20967.20795006453, 21194.427318009974, 21088.521528526755, 19437.91555757985, 13677.902713248171, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 871528441401.8333, 1083221770553.0684, 147034752676.7702, 35556045575.13566, 35153369257.41337, 46086368691.51654, 24689305171.692936, 11343276593.440475, 5396624651.94418, 2449353007.641508, 1132225885.703891, 579547849.1340877, 330219246.7861086, 207613930.3131764, 144580292.27473342, 109933282.92266414, 88706603.092171, 73819777.54163922, 63615988.74519494, 57250262.292053565, 52976073.06761927, 49653169.17819005, 46544975.11484598, 43167606.9599748, 39724375.20499403, 36317177.25886468, 33057511.80930482, 29869089.497658804, 26982386.85583376, 24416235.17215712, 22273651.697369896, 20553426.04804544, 19216240.03357431, 18167694.44812838, 17501855.536957663, 17169938.630597908, 17005382.258644175, 16998475.26752617, 17082890.987979066, 17227982.77516062, 17445823.21630204, 17757404.421785507, 18346092.75160569, 19400573.66632694, 20506722.48296608, 22469648.380506545, 23432031.455169585, 26204163.40545158, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 1000000000000000.0, 3673829810926.31, 371405570725.2526, 14219163611.984406, 3001863018.1934915, 1432766589.9326108, 884599805.0283787, 560127980.1033351, 386052567.7087711, 287331851.051439, 222703657.59538063, 181069239.6264349, 154620864.3164144, 138093777.60284117, 126605828.89875436, 117967840.02553518, 111005814.39518328, 105186901.20678852, 100168133.0295481, 95568646.67416307, 91457433.39515457, 88871610.45308323, 88829796.26374224, 91398113.73291488, 96585131.67000748, 104507692.01463065, 115895119.998433, 131939701.08213414, 154492946.00677127, 183147918.17086875, 215151374.22324687, 247158314.6345976, 266792879.42215955, 279115128.29108113, 370541510.87006927, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 877670509694.7871, 1174826943136.8308, 1270605570069.038, 21727315470.5208, 3159456646.5437946, 1090653401.282219, 727967089.8459107, 384399548.9506704, 290787296.9451616, 232703218.45048887, 197467462.7577736, 174310890.8025987, 160536437.73297343, 153567098.77048483, 152120124.9453068, 153115566.6756177, 153955545.42558223, 153734675.21565756, 154798666.36905554, 163346213.58113608, 180013139.3707387, 200324358.8534948, 220754613.1646765, 241290935.478592, 262868932.2066308, 284448910.01847774, 305681084.4142859, 327605088.8575117, 350473296.7263526, 373964594.1196182, 398396925.8173239, 423528355.65716046, 450447055.544388, 478857006.4973163, 508200335.7126168, 537309657.5789208, 566854568.2904652, 594618842.9455439, 619715928.2391286, 641395460.8414665, 663290039.7810476, 689274894.631561, 718208866.3397261, 743951200.8024124, 761776104.2945968, 772911224.3082078, 804001144.8046833, 772448774.7758856, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4613823.568205323, 1999308.9343799097, 904636.2296014762, 433823.6123842511, 207201.39055371704, 107836.09164720173, 57647.915219220784, 40606.52305039815, 47739.86647922776, 51669.35493930698, 56438.19768395407, 60447.45665200092, 65251.4153955275, 71920.88588011517, 78529.58115204438, 83422.30217897324, 87036.98552475807, 90389.72631774022, 93982.39165674087, 97578.0099352472, 101428.21366062944, 104630.69200130588, 105685.04322626138, 103962.58423268417, 99650.31670632094, 94290.49986206587, 89514.90144353417, 85905.45713126978, 82784.9857650212, 79152.28707014346, 74847.81017353121, 70378.81859610273, 65420.04643792357, 59953.75184604176, 54764.28281143022, 50362.51288353384, 46212.571031725325, 41997.52779088816, 37692.05148110484, 33834.73460995647, 31846.09764364542, 31934.145655397457, 31454.81247448105, 30105.4073072481, 26957.830283611693, 27760.04479210889, 29853.374336459365, 19133.428743715107, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7619940.584531054, 3148394.472742347, 1308415.0022178134, 540515.7720745018, 215237.1053603881, 102546.7276372816, 68453.67122640925, 50692.59053608593, 51487.52043139844, 52104.76838400132, 54019.39151917722, 55856.02168787862, 60347.30240270209, 68990.96019017675, 79096.88768563846, 87574.33453690328, 94158.56052476274, 101903.63670531697, 111746.9753834774, 122460.65399236557, 132086.69387474353, 141041.48571028374, 146354.09441287292, 145953.09590059065, 139496.8007888401, 128508.85108217449, 116665.51769667884, 107458.39706309135, 100259.97236694951, 94108.98505029618, 88439.89456238014, 82734.9027659809, 77061.08621371102, 71333.5319243128, 65999.72532130677, 61798.9972058361, 58237.356419617165, 54715.10266341248, 50825.84431702935, 46059.17688689915, 40740.26050401376, 36335.80228304863, 33981.57568605091, 33589.7143390849, 33988.88524112733, 36272.9364507092, 41183.34413717943, 29194.12369278645, 0.0040536134869726, 0.0138824238058072, 135129884.5084534, 12219717.5342461, 0.0090705273332672, 0.0085898851680217, 0.0215368188774867, 0.0336321308942602]\n",
    "TARGET_WEIGHTS = np.array(TARGET_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECA(nn.Module):\n",
    "    def __init__(self, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.supports_masking = True\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=kernel_size, stride=1, padding=\"same\", bias=False)\n",
    "    def forward(self, inputs):\n",
    "        b, c, s = inputs.shape\n",
    "        \n",
    "        x = torch.mean(inputs, axis = -1)\n",
    "        x = x.view(b, 1, c)\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = x[:,:,None]\n",
    "        return inputs * x\n",
    "\n",
    "\n",
    "class CausalDWConv1D(nn.Module):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        in_channels = 64,\n",
    "        out_channels = 32,       \n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        **kwargs):\n",
    "        super().__init__()\n",
    "        #self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            stride=1, \n",
    "            padding='same', \n",
    "            dilation=dilation_rate, \n",
    "            groups=out_channels if kernel_size > 3 else 1,\n",
    "            bias=False, \n",
    "            padding_mode='zeros')\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.dw_conv(inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv1DBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 kernel_size=17,\n",
    "                 channels = 32,\n",
    "                 expand_channels = 64,\n",
    "                 drop_rate=0.0,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = CausalDWConv1D(\n",
    "                        kernel_size=kernel_size,\n",
    "                        dilation_rate=1,\n",
    "                        use_bias=False,\n",
    "                        in_channels = expand_channels,\n",
    "                        out_channels = expand_channels\n",
    "                    )\n",
    "        self.dnn_expand = nn.Linear(in_features = channels, \n",
    "                                    out_features = expand_channels\n",
    "                                     )\n",
    "        self.dnn_project = nn.Linear(in_features = expand_channels, \n",
    "                             out_features = channels\n",
    "                                    )\n",
    "        self.bn = nn.BatchNorm1d(num_features = expand_channels, eps=0.95)\n",
    "        self.eca = ECA()\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        skip = inputs\n",
    "\n",
    "        x = inputs.permute([0,2,1])\n",
    "        x = self.dnn_expand(x)\n",
    "        \n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.act(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.eca(x)\n",
    "        \n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.dnn_project(x)\n",
    "        x = x.permute([0,2,1])\n",
    "\n",
    "        return x + skip\n",
    "\n",
    "\n",
    "class Conv1DModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 kernel_size=17,\n",
    "                 channels = 32,\n",
    "                 expand_channels = 64,\n",
    "                 drop_rate=0.0,\n",
    "                 num_blocks_in_stage = 3,\n",
    "                 input_len = 32_000*5,\n",
    "                 n_classes = 182\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.stem_conv = nn.Linear(in_features = 1, \n",
    "                                    out_features = channels\n",
    "                                     )\n",
    "        self.stem_bn = nn.BatchNorm1d(num_features = channels, eps=0.95)\n",
    "\n",
    "        self.ConvStage_1 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                         for _ in range(num_blocks_in_stage)])\n",
    "        self.PoolStage_1 = nn.AvgPool1d(kernel_size=(4))\n",
    "        \n",
    "        self.ConvStage_2 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                          for _ in range(num_blocks_in_stage)])\n",
    "        self.PoolStage_2 = nn.AvgPool1d(kernel_size=(4))\n",
    "\n",
    "        \n",
    "        self.ConvStage_3 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                          for _ in range(num_blocks_in_stage)])\n",
    "        self.PoolStage_3 = nn.AvgPool1d(kernel_size=(4))\n",
    "\n",
    "        self.pre_out = nn.Linear(in_features = channels, out_features = n_classes*2)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.out_act = nn.SiLU()\n",
    "        self.out = nn.Linear(in_features = n_classes*2, out_features = n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        b, s = inputs.shape\n",
    "        x = inputs.view(b, s, 1)\n",
    "        x = self.stem_conv(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.stem_bn(x)\n",
    "\n",
    "        for block in self.ConvStage_1:\n",
    "            x = block(x)\n",
    "        x = self.PoolStage_1(x)\n",
    "\n",
    "        for block in self.ConvStage_2:\n",
    "            x = block(x)\n",
    "        x = self.PoolStage_2(x)\n",
    "\n",
    "        for block in self.ConvStage_3:\n",
    "            x = block(x)\n",
    "        x = self.PoolStage_3(x)\n",
    "\n",
    "        x = x.mean(axis=2)\n",
    "\n",
    "        x = self.pre_out(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_act(x)\n",
    "        \n",
    "        logits = self.out(x)\n",
    "        probs = self.sigmoid(logits)\n",
    "\n",
    "        return {\n",
    "                \"clipwise_logits_long\": logits,\n",
    "                \"clipwise_pred_long\": probs,\n",
    "            }\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = nn.MultiheadAttention(n_embd, n_head)\n",
    "        self.ffwd = FeedFoward(n_embd, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, q = None):\n",
    "        if q is not None:\n",
    "            X = (q, x, x)\n",
    "        else:\n",
    "            X = (x, x, x)\n",
    "        y = self.sa(*X)\n",
    "        y = y[0]\n",
    "        \n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTransBlock(nn.Module):\n",
    "    def __init__(self, block_kernels = [5, 3], n_head = 4, channels=16, expand_channels=32, drop_rate = 0.1, att_drop_rate = 0.25, n_features=25):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(*[\n",
    "            Conv1DBlock(kernel_size=k, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "            for k in block_kernels\n",
    "        ])\n",
    "\n",
    "        self.block = Block(n_embd = channels, n_head=n_head, dropout = att_drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        x = self.block(x)\n",
    "        x = x.permute([0,2,1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExctractor(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size = 7, channels=16, expand_channels=32, drop_rate = 0.1, n_features=25):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.Scales60 = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                out_channels = channels,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(9)])\n",
    "        \n",
    "        self.ScalesFlat = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                out_channels = channels//2,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(16)])\n",
    "        \n",
    "        self.ConvExt60 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                          for _ in range(9)])\n",
    "        self.ConvExtFlat = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels//2,expand_channels = expand_channels//2, drop_rate=drop_rate)\n",
    "                                          for _ in range(16)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 556)\n",
    "        \n",
    "        state_t = x[:, 0:60] - 273\n",
    "        state_q0001 = x[:, 60:120] *1_000\n",
    "        state_q0002 = x[:, 120:180] *1_000\n",
    "        state_q0003 = x[:, 180:240] *1_000\n",
    "        state_u = x[:, 240:300] / 100\n",
    "        state_v = x[:, 300:360] / 100\n",
    "    \n",
    "        state_ps = x[:, 360:361]/ 100_000 - 1\n",
    "        pbuf_SOLIN = x[:, 361:362] / 1000\n",
    "        pbuf_LHFLX = x[:, 362:363] / 1000\n",
    "        pbuf_SHFLX = x[:, 363:364] / 1000\n",
    "        pbuf_TAUX = x[:, 364:365] / 1\n",
    "        pbuf_TAUY = x[:, 365:366] / 1\n",
    "        pbuf_COSZRS = x[:, 366:367] / 1\n",
    "        cam_in_ALDIF = x[:, 367:368] / 1\n",
    "        cam_in_ALDIR = x[:, 368:369] / 1\n",
    "        cam_in_ASDIF = x[:, 369:370] / 1\n",
    "        cam_in_ASDIR = x[:, 370:371] / 1\n",
    "        cam_in_LWUP = x[:, 371:372] / 1000\n",
    "        cam_in_ICEFRAC = x[:, 372:373] / 1\n",
    "        cam_in_LANDFRAC = x[:, 373:374] /1\n",
    "        cam_in_OCNFRAC = x[:, 374:375]  /1\n",
    "        cam_in_SNOWHLAND = x[:, 375:376] / 1\n",
    "    \n",
    "        pbuf_ozone = x[:, 376:436] * 100_000\n",
    "        pbuf_CH4 = x[:, 436:496] * 100_000\n",
    "        pbuf_N2O = x[:, 496:556] * 100_000\n",
    "            \n",
    "        inputs_60 = [\n",
    "                state_t,\n",
    "                state_q0001,\n",
    "                state_q0002,\n",
    "                state_q0003, \n",
    "                state_u,\n",
    "                state_v,\n",
    "    \n",
    "                pbuf_ozone,\n",
    "                pbuf_CH4,\n",
    "                pbuf_N2O\n",
    "        ]\n",
    "\n",
    "        inputs_flat = [            \n",
    "                torch.repeat_interleave(state_ps, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SOLIN, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_LHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_TAUX, 60, dim=-1),\n",
    "               torch.repeat_interleave(pbuf_TAUY, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_COSZRS, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIR, 60, dim=-1),\n",
    "               torch.repeat_interleave(cam_in_ASDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ASDIR, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LWUP, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ICEFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LANDFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_OCNFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_SNOWHLAND, 60, dim=-1),\n",
    "        ]\n",
    "        \n",
    "        output = []\n",
    "        for i, conv in enumerate(self.ConvExt60):\n",
    "            t = inputs_60[i]\n",
    "            t = t.view(-1, 1, 60)\n",
    "            t = self.Scales60[i](t)\n",
    "            output.append(conv(t))\n",
    "            \n",
    "        for i, conv in enumerate(self.ConvExtFlat):\n",
    "            t = inputs_flat[i]\n",
    "            t = t.view(-1, 1, 60)\n",
    "            t = self.ScalesFlat[i](t)\n",
    "            output.append(conv(t))\n",
    "\n",
    "\n",
    "        return torch.cat(output, 1)#.permute([0,2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExctractor(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size = 7, channels=16, expand_channels=32, drop_rate = 0.1, n_features=25):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.Scales60 = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                out_channels = channels,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(9)])\n",
    "        \n",
    "        self.ScalesFlat = nn.ModuleList([nn.Conv1d(in_channels = 1,\n",
    "                                                out_channels = channels//2,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same') for _ in range(16)])\n",
    "        \n",
    "        self.ConvExt60 = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels,expand_channels = expand_channels, drop_rate=drop_rate)\n",
    "                                          for _ in range(9)])\n",
    "        self.ConvExtFlat = nn.ModuleList([\n",
    "            Conv1DBlock(kernel_size=kernel_size, channels = channels//2,expand_channels = expand_channels//2, drop_rate=drop_rate)\n",
    "                                          for _ in range(16)])\n",
    "        \n",
    "        self.emb  = nn.Embedding(385, int(channels//2 * 16 + channels*9) , max_norm=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 557)\n",
    "        \n",
    "        state_t = x[:, 0:60] - 273\n",
    "        state_q0001 = x[:, 60:120] *1_000\n",
    "        state_q0002 = x[:, 120:180] *1_000\n",
    "        state_q0003 = x[:, 180:240] *1_000\n",
    "        state_u = x[:, 240:300] / 100\n",
    "        state_v = x[:, 300:360] / 100\n",
    "    \n",
    "        state_ps = x[:, 360:361]/ 100_000 - 1\n",
    "        pbuf_SOLIN = x[:, 361:362] / 1000\n",
    "        pbuf_LHFLX = x[:, 362:363] / 1000\n",
    "        pbuf_SHFLX = x[:, 363:364] / 1000\n",
    "        pbuf_TAUX = x[:, 364:365] / 1\n",
    "        pbuf_TAUY = x[:, 365:366] / 1\n",
    "        pbuf_COSZRS = x[:, 366:367] / 1\n",
    "        cam_in_ALDIF = x[:, 367:368] / 1\n",
    "        cam_in_ALDIR = x[:, 368:369] / 1\n",
    "        cam_in_ASDIF = x[:, 369:370] / 1\n",
    "        cam_in_ASDIR = x[:, 370:371] / 1\n",
    "        cam_in_LWUP = x[:, 371:372] / 1000\n",
    "        cam_in_ICEFRAC = x[:, 372:373] / 1\n",
    "        cam_in_LANDFRAC = x[:, 373:374] /1\n",
    "        cam_in_OCNFRAC = x[:, 374:375]  /1\n",
    "        cam_in_SNOWHLAND = x[:, 375:376] / 1\n",
    "    \n",
    "        pbuf_ozone = x[:, 376:436] * 100_000\n",
    "        pbuf_CH4 = x[:, 436:496] * 100_000\n",
    "        pbuf_N2O = x[:, 496:556] * 100_000\n",
    "            \n",
    "        inputs_60 = [\n",
    "                state_t,\n",
    "                state_q0001,\n",
    "                state_q0002,\n",
    "                state_q0003, \n",
    "                state_u,\n",
    "                state_v,\n",
    "    \n",
    "                pbuf_ozone,\n",
    "                pbuf_CH4,\n",
    "                pbuf_N2O\n",
    "        ]\n",
    "\n",
    "        inputs_flat = [            \n",
    "                torch.repeat_interleave(state_ps, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SOLIN, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_LHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_SHFLX, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_TAUX, 60, dim=-1),\n",
    "               torch.repeat_interleave(pbuf_TAUY, 60, dim=-1),\n",
    "                torch.repeat_interleave(pbuf_COSZRS, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ALDIR, 60, dim=-1),\n",
    "               torch.repeat_interleave(cam_in_ASDIF, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ASDIR, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LWUP, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_ICEFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_LANDFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_OCNFRAC, 60, dim=-1),\n",
    "                torch.repeat_interleave(cam_in_SNOWHLAND, 60, dim=-1),\n",
    "        ]\n",
    "\n",
    "        pos = x[:, 556:557]\n",
    "        pos = torch.repeat_interleave(pos, 60, dim=1)       \n",
    "        \n",
    "        output = []\n",
    "        for i, conv in enumerate(self.ConvExt60):\n",
    "            t = inputs_60[i]\n",
    "            t = t.view(-1, 1, 60)\n",
    "            t = self.Scales60[i](t)\n",
    "            output.append(conv(t))\n",
    "            \n",
    "        for i, conv in enumerate(self.ConvExtFlat):\n",
    "            t = inputs_flat[i]\n",
    "            t = t.view(-1, 1, 60)\n",
    "            t = self.ScalesFlat[i](t)\n",
    "            output.append(conv(t))\n",
    "\n",
    "        x = torch.cat(output, 1)\n",
    "        x_pos = self.emb(pos.long()).squeeze()\n",
    "\n",
    "        \n",
    "        return x + x_pos.permute([0,2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 272, 60])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FeatureExctractor()(torch.ones([8,557])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEADHead(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "\n",
    "        self.act = nn.SELU()\n",
    "        self.conv_seq = nn.Conv1d(in_channels = n_embd, out_channels = 6,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same')\n",
    "        \n",
    "        self.conv_flat = nn.Conv1d(in_channels = n_embd, out_channels = 8,\n",
    "                                                kernel_size = 1,\n",
    "                                                stride=1, \n",
    "                                                padding='same')\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        #x = x.permute([0,2,1])\n",
    "        \n",
    "        p_seq = self.conv_seq(x)\n",
    "        p_seq = nn.Flatten()(p_seq)\n",
    "    \n",
    "        p_flat = self.conv_flat(x)\n",
    "        p_flat = torch.mean(p_flat, axis = -1)\n",
    "        \n",
    "        return torch.cat([p_seq, p_flat], axis= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2465, -1.2465, -1.2465,  ..., -1.0379, -0.0395,  0.4833],\n",
       "        [-1.2465, -1.2465, -1.2465,  ..., -1.0379, -0.0395,  0.4833],\n",
       "        [-1.2465, -1.2465, -1.2465,  ..., -1.0379, -0.0395,  0.4833],\n",
       "        ...,\n",
       "        [-1.2465, -1.2465, -1.2465,  ..., -1.0379, -0.0395,  0.4833],\n",
       "        [-1.2465, -1.2465, -1.2465,  ..., -1.0379, -0.0395,  0.4833],\n",
       "        [-1.2465, -1.2465, -1.2465,  ..., -1.0379, -0.0395,  0.4833]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEADHead(32)(torch.ones([8,32,60]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# block_size = 256\n",
    "# max_iters = 5000\n",
    "# learning_rate = 3e-4\n",
    "# eval_iters = 100\n",
    "# n_embd = 384\n",
    "# n_head = 8\n",
    "# n_layer = 12\n",
    "# dropout = 0.2\n",
    "\n",
    "nn_config = dict(\n",
    "    n_embd = 256,\n",
    "    n_head = 4,\n",
    "    fe_channels = 32, \n",
    "    encoder_layers = 3, \n",
    "    fe_drop_rate = 0.1,\n",
    "    att_drop_rate = 0.2,\n",
    "    n_features = 25,\n",
    "    bottleneck_k_size = 3,\n",
    "    block_kernels = [5, 3]\n",
    ")\n",
    "\n",
    "    \n",
    "class LEADModelAtt(nn.Module):\n",
    "    def __init__(self, n_embd = 64, n_head = 4, encoder_layers = 3, fe_channels=16, fe_drop_rate=0.1, \n",
    "                 att_drop_rate=0.2, n_features = 25, bottleneck_k_size = 3, block_kernels = [5, 3]):\n",
    "        super().__init__()\n",
    "        self.fe = FeatureExctractor(kernel_size = 7, channels=fe_channels, expand_channels=fe_channels*2, drop_rate = fe_drop_rate, n_features=n_features)\n",
    "        self.linearStem = nn.Linear(fe_channels*9 + fe_channels//2 * 16, n_embd)\n",
    "        self.bottleneck = Conv1DBlock(kernel_size=bottleneck_k_size, channels = n_embd, expand_channels = n_embd*2, drop_rate=fe_drop_rate)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[ConvTransBlock(block_kernels = block_kernels, \n",
    "                                                     channels = n_embd, \n",
    "                                                     expand_channels = n_embd*2, \n",
    "                                                     n_head=n_head, \n",
    "                                                     drop_rate = fe_drop_rate, \n",
    "                                                     att_drop_rate = att_drop_rate) for _ in range(encoder_layers)])\n",
    "        \n",
    "        self.head  = LEADHead(n_embd = n_embd)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        #B, T = inputs.shape\n",
    "\n",
    "        xf = self.fe(inputs)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.linearStem(xf)\n",
    "        xf = xf.permute([0,2,1])\n",
    "        xf = self.bottleneck(xf)\n",
    "        x = xf#.permute([0,2,1])\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "\n",
    "        out = self.head(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_channels = 32\n",
    "fe_channels*9 + fe_channels//2 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 368])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEADModelAtt(**nn_config)(torch.ones([8, 557])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "class FocalLossBCE(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha: float = 0.25,\n",
    "            gamma: float = 2,\n",
    "            reduction: str = \"mean\",\n",
    "            bce_weight: float = 1.0,\n",
    "            focal_weight: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss(reduction=reduction) #, pos_weight=sample_weights_420)\n",
    "        self.bce_weight = bce_weight\n",
    "        self.focal_weight = focal_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        focall_loss = torchvision.ops.focal_loss.sigmoid_focal_loss(\n",
    "            inputs=logits,\n",
    "            targets=targets,\n",
    "            alpha=self.alpha,\n",
    "            gamma=self.gamma,\n",
    "            reduction=self.reduction,\n",
    "        )\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "        return self.bce_weight * bce_loss + self.focal_weight * focall_loss\n",
    "\n",
    "\n",
    "criterion = FocalLossBCE(focal_weight=5, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"train_data_sample.parquet\").sample(200000)\n",
    "df['pos'] = (df.index % 384).to_list()\n",
    "\n",
    "df = df.drop('sample_id', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, 556:924].to_numpy() * TARGET_WEIGHTS\n",
    "\n",
    "mean_y = y.mean(0)\n",
    "std_y = df.iloc[:, 556:924].std().to_numpy()\n",
    "std_y = np.clip(std_y, 1e-10, 1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_y = torch.maximum(torch.sqrt(torch.mean(torch.pow(torch.tensor(y), 2), 0)), torch.tensor(1e-10)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEAD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, augmentation=False, mode='train'):\n",
    "        if mode == 'train':\n",
    "            self.df = df.reset_index(drop=True)\n",
    "        elif mode == 'valid':\n",
    "            self.df = df.reset_index(drop=True)\n",
    "        else:\n",
    "            self.df = df.reset_index(drop=True)\n",
    "        self.mode = mode\n",
    "        self.augmentation = augmentation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.df.iloc[idx, :556].to_numpy() \n",
    "        y = self.df.iloc[idx, 556:924].to_numpy() * TARGET_WEIGHTS\n",
    "        \n",
    "        pos = self.df.iloc[idx, 924:].to_numpy() \n",
    "        \n",
    "        y = (y - mean_y) / std_y\n",
    "        \n",
    "        return torch.cat([torch.tensor(x), torch.tensor(pos)], -1).float(), torch.tensor(y)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2394e+02,  2.3503e+02,  2.3989e+02,  2.4910e+02,  2.5971e+02,\n",
       "          2.6623e+02,  2.6409e+02,  2.5529e+02,  2.4416e+02,  2.3618e+02,\n",
       "          2.2987e+02,  2.2462e+02,  2.2019e+02,  2.1663e+02,  2.1381e+02,\n",
       "          2.1161e+02,  2.0894e+02,  2.0685e+02,  2.0652e+02,  2.0863e+02,\n",
       "          2.1057e+02,  2.1351e+02,  2.1750e+02,  2.2186e+02,  2.2602e+02,\n",
       "          2.2998e+02,  2.3367e+02,  2.3709e+02,  2.4037e+02,  2.4352e+02,\n",
       "          2.4655e+02,  2.4945e+02,  2.5247e+02,  2.5539e+02,  2.5818e+02,\n",
       "          2.6080e+02,  2.6344e+02,  2.6589e+02,  2.6838e+02,  2.7051e+02,\n",
       "          2.7258e+02,  2.7437e+02,  2.7624e+02,  2.7754e+02,  2.7865e+02,\n",
       "          2.7968e+02,  2.8091e+02,  2.8205e+02,  2.8295e+02,  2.8380e+02,\n",
       "          2.8344e+02,  2.8443e+02,  2.8518e+02,  2.8568e+02,  2.8602e+02,\n",
       "          2.8613e+02,  2.8620e+02,  2.8607e+02,  2.8516e+02,  2.8601e+02,\n",
       "          1.1173e-06,  1.0842e-06,  1.0669e-06,  1.0953e-06,  1.0951e-06,\n",
       "          1.0896e-06,  1.0900e-06,  1.0949e-06,  1.1116e-06,  1.1414e-06,\n",
       "          1.1946e-06,  1.2520e-06,  1.3477e-06,  1.6112e-06,  1.8148e-06,\n",
       "          1.9188e-06,  2.2742e-06,  2.7517e-06,  2.9186e-06,  3.8471e-06,\n",
       "          1.2132e-05,  3.2581e-05,  5.9784e-05,  9.4138e-05,  1.4678e-04,\n",
       "          2.1768e-04,  3.0819e-04,  4.1172e-04,  5.3409e-04,  6.7728e-04,\n",
       "          8.4313e-04,  1.0290e-03,  1.1820e-03,  1.3735e-03,  1.7117e-03,\n",
       "          2.1505e-03,  2.7094e-03,  3.4024e-03,  3.9938e-03,  4.5634e-03,\n",
       "          5.1069e-03,  5.6833e-03,  6.2186e-03,  6.7338e-03,  7.1550e-03,\n",
       "          7.5308e-03,  7.8893e-03,  8.3048e-03,  8.6392e-03,  8.8615e-03,\n",
       "          9.0306e-03,  9.3464e-03,  9.5851e-03,  9.7744e-03,  9.8701e-03,\n",
       "          9.8843e-03,  9.7799e-03,  9.4123e-03,  8.3894e-03,  8.3293e-03,\n",
       "          1.9332e-34,  1.3021e-34,  8.5484e-35,  1.4457e-34,  1.2064e-34,\n",
       "          6.8363e-35,  3.3413e-35,  1.7268e-35,  7.3774e-36,  2.3889e-36,\n",
       "          5.1035e-37,  8.5509e-38,  1.8871e-41,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  1.4013e-45,  6.2274e-42,  0.0000e+00,  2.3939e-32,\n",
       "          0.0000e+00,  1.0336e-22,  3.5483e-17,  2.7661e-12,  4.2032e-09,\n",
       "          1.9035e-11,  4.3812e-08,  6.6712e-08,  1.0444e-06,  1.6258e-06,\n",
       "          5.7790e-07,  5.6106e-07,  4.4446e-06,  9.9978e-06,  1.2014e-05,\n",
       "          5.0561e-06,  1.2386e-05,  1.4336e-05,  2.8441e-05,  1.2579e-05,\n",
       "          2.9647e-05,  3.3941e-06,  3.4377e-06,  5.0462e-06,  5.6087e-06,\n",
       "          6.1246e-05,  1.1211e-05,  6.1927e-06,  8.4654e-06,  1.0709e-05,\n",
       "          1.1213e-05,  1.0185e-05,  9.2737e-06,  7.3035e-06,  1.7049e-06,\n",
       "          9.2878e-21,  2.1340e-20,  1.1007e-19,  9.0915e-20,  1.5568e-19,\n",
       "          4.7464e-19,  1.2074e-18,  3.4059e-18,  1.2478e-18,  2.3268e-18,\n",
       "          1.9734e-18,  1.4876e-18,  6.2805e-22,  2.9487e-27,  1.1575e-33,\n",
       "          1.0214e-32,  7.1263e-14,  2.3782e-11,  7.3588e-11,  1.2808e-10,\n",
       "          3.5333e-10,  1.0894e-08,  1.5581e-08,  3.7060e-08,  6.6364e-08,\n",
       "          1.2653e-07,  2.7848e-07,  5.2331e-07,  4.4661e-07,  2.9752e-07,\n",
       "          1.3189e-06,  1.0162e-06,  7.9527e-07,  1.4643e-06,  1.5609e-06,\n",
       "          1.1869e-07,  1.3508e-07,  5.4708e-07,  7.3072e-08,  4.2632e-07,\n",
       "          6.3095e-09,  7.7063e-08,  0.0000e+00,  3.1635e-08,  0.0000e+00,\n",
       "          7.7133e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          3.6120e-09,  3.2551e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "         -3.6784e+01, -2.3645e+01, -9.5354e+00, -4.5344e-01,  6.8538e-01,\n",
       "          3.1163e+00,  6.6418e+00,  8.4025e+00,  8.0241e+00,  6.6426e+00,\n",
       "          4.9312e+00,  3.2636e+00,  2.0241e+00,  1.7671e+00,  2.7420e+00,\n",
       "          4.4995e+00,  7.2269e+00,  1.0807e+01,  1.3809e+01,  1.5698e+01,\n",
       "          1.6919e+01,  1.7287e+01,  1.7251e+01,  1.6930e+01,  1.6459e+01,\n",
       "          1.5942e+01,  1.5297e+01,  1.4589e+01,  1.3827e+01,  1.3052e+01,\n",
       "          1.2273e+01,  1.1508e+01,  1.0749e+01,  9.9877e+00,  9.2274e+00,\n",
       "          8.5005e+00,  7.7405e+00,  6.9606e+00,  6.1405e+00,  5.3937e+00,\n",
       "          4.6642e+00,  3.9348e+00,  3.3050e+00,  2.7464e+00,  2.2558e+00,\n",
       "          1.7755e+00,  1.2464e+00,  7.1212e-01,  2.4946e-01, -1.2360e-01,\n",
       "         -4.4846e-01, -7.3501e-01, -1.0638e+00, -1.4273e+00, -1.8251e+00,\n",
       "         -2.1052e+00, -2.4317e+00, -2.4998e+00, -2.0293e+00, -3.8791e-01,\n",
       "          1.6224e+01,  2.8219e+01,  1.9249e+01,  1.1301e+01,  6.6305e+00,\n",
       "          5.5285e+00,  5.4068e+00,  4.9260e+00,  3.4975e+00,  1.7971e+00,\n",
       "          5.5511e-01, -7.4973e-02, -1.9820e-01,  1.2933e-01,  1.0842e+00,\n",
       "          2.5630e+00,  4.3360e+00,  6.6892e+00,  8.9252e+00,  1.0815e+01,\n",
       "          1.2298e+01,  1.3330e+01,  1.4081e+01,  1.4381e+01,  1.4377e+01,\n",
       "          1.3979e+01,  1.3271e+01,  1.2382e+01,  1.1358e+01,  1.0291e+01,\n",
       "          9.2121e+00,  8.1740e+00,  7.1672e+00,  6.1830e+00,  5.1918e+00,\n",
       "          4.1665e+00,  3.1317e+00,  2.1121e+00,  1.1480e+00,  2.6311e-01,\n",
       "         -7.0169e-01, -1.5675e+00, -2.3353e+00, -2.9885e+00, -3.5854e+00,\n",
       "         -4.2489e+00, -4.9095e+00, -5.4451e+00, -5.8519e+00, -6.2146e+00,\n",
       "         -6.5397e+00, -6.8947e+00, -7.1985e+00, -7.4917e+00, -7.7886e+00,\n",
       "         -8.2675e+00, -8.6387e+00, -9.1185e+00, -9.7709e+00, -8.7200e+00,\n",
       "          1.0041e+05,  7.3942e+02,  8.8563e+00, -2.0211e+00,  3.8112e-03,\n",
       "          7.3376e-02,  5.5402e-01,  6.1656e-02,  6.3994e-02,  5.9785e-02,\n",
       "          6.2430e-02,  3.7681e+02,  0.0000e+00,  1.3183e-02,  9.8682e-01,\n",
       "          0.0000e+00,  2.7029e-07,  4.8680e-07,  8.7255e-07,  1.5498e-06,\n",
       "          2.7128e-06,  4.6482e-06,  7.7442e-06,  1.2230e-05,  1.2278e-05,\n",
       "          1.2084e-05,  1.1211e-05,  1.0116e-05,  8.7559e-06,  7.0148e-06,\n",
       "          5.2166e-06,  3.8824e-06,  2.7055e-06,  1.7008e-06,  1.0140e-06,\n",
       "          6.5577e-07,  4.8539e-07,  3.8508e-07,  3.0674e-07,  2.3992e-07,\n",
       "          2.0198e-07,  1.7339e-07,  1.5445e-07,  1.4309e-07,  1.3396e-07,\n",
       "          1.2817e-07,  1.2325e-07,  1.1882e-07,  1.1478e-07,  1.1066e-07,\n",
       "          1.0686e-07,  1.0298e-07,  9.8968e-08,  9.4848e-08,  9.0752e-08,\n",
       "          8.6753e-08,  8.2914e-08,  7.9250e-08,  7.5774e-08,  7.2464e-08,\n",
       "          6.9312e-08,  6.6316e-08,  6.3472e-08,  6.1099e-08,  5.9199e-08,\n",
       "          5.7423e-08,  5.5762e-08,  5.4195e-08,  5.3408e-08,  5.3000e-08,\n",
       "          5.2597e-08,  5.2194e-08,  5.1830e-08,  5.1725e-08,  5.1619e-08,\n",
       "          5.0974e-08,  8.3876e-08,  1.0119e-07,  1.2190e-07,  1.4641e-07,\n",
       "          1.7504e-07,  2.0784e-07,  2.4459e-07,  2.8478e-07,  3.2770e-07,\n",
       "          3.7266e-07,  4.1911e-07,  4.6673e-07,  5.1531e-07,  5.6464e-07,\n",
       "          6.1436e-07,  6.6395e-07,  7.1283e-07,  7.6052e-07,  8.0678e-07,\n",
       "          8.5156e-07,  8.9493e-07,  9.3691e-07,  9.7741e-07,  9.9861e-07,\n",
       "          9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,\n",
       "          9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,\n",
       "          9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,\n",
       "          9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,\n",
       "          9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,\n",
       "          9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,\n",
       "          9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,  9.9861e-07,\n",
       "          9.9861e-07,  1.4966e-08,  1.9497e-08,  2.5344e-08,  3.2812e-08,\n",
       "          4.2199e-08,  5.3756e-08,  6.7620e-08,  8.3785e-08,  1.0211e-07,\n",
       "          1.2239e-07,  1.4442e-07,  1.6807e-07,  1.9323e-07,  2.1980e-07,\n",
       "          2.4756e-07,  2.7617e-07,  3.0525e-07,  3.3442e-07,  3.6343e-07,\n",
       "          3.9217e-07,  4.2060e-07,  4.4867e-07,  4.7624e-07,  4.9086e-07,\n",
       "          4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,\n",
       "          4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,\n",
       "          4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,\n",
       "          4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,\n",
       "          4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,\n",
       "          4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,\n",
       "          4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,  4.9086e-07,\n",
       "          4.9086e-07,  3.7400e+02]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEAD_Dataset(df).__getitem__(3)[0].view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0727e-01,  2.5569e-01,  2.6243e-01,  2.1243e-01,  2.0033e-01,\n",
       "          3.7126e-01,  4.8705e-01,  2.7588e-01,  3.1144e-01,  3.6225e-01,\n",
       "          2.1717e-01,  3.3029e-01,  2.8145e-01,  3.4692e-01,  2.8682e-01,\n",
       "          3.5745e-01,  4.0530e-01,  2.7902e-01,  2.6630e-01,  2.8650e-01,\n",
       "          3.1841e-01,  3.4007e-01,  2.0181e-01,  3.4871e-01,  2.4535e-01,\n",
       "          1.7324e-01,  2.4345e-01,  2.0873e-01,  3.9702e-01,  2.8750e-01,\n",
       "          2.4002e-01,  2.3984e-01,  2.6250e-01,  2.6552e-01,  5.0149e-01,\n",
       "          4.9482e-01,  3.7351e-01,  4.0266e-01,  5.2865e-01,  7.2146e-01,\n",
       "          5.6031e-01,  5.7967e-01,  3.4545e-01,  3.7563e-01,  3.4779e-01,\n",
       "          2.6583e-01,  2.9614e-01,  2.4823e-01,  2.7815e-01,  2.9194e-01,\n",
       "          2.0267e-01,  4.1963e-01,  1.7632e-01,  2.2962e-01,  1.5188e-01,\n",
       "          1.7362e-01,  1.5981e-01,  1.9565e-01,  1.2137e-01,  4.6855e-02,\n",
       "          1.0724e+00,  1.0687e+00,  1.0417e+00,  1.0586e+00,  1.1666e+00,\n",
       "          1.2827e+00,  1.2685e+00,  1.1304e+00,  1.1073e+00,  1.0107e+00,\n",
       "          1.0980e+00,  1.0339e+00,  1.1110e+00,  1.0481e+00,  1.1172e+00,\n",
       "          1.1807e+00,  1.0478e+00,  1.1028e+00,  9.2556e-01,  1.1855e+00,\n",
       "          1.0628e+00,  1.1572e+00,  9.5622e-01,  1.0456e+00,  9.9294e-01,\n",
       "          1.1186e+00,  1.0762e+00,  1.0817e+00,  1.0422e+00,  1.0701e+00,\n",
       "          1.1411e+00,  1.1625e+00,  1.1539e+00,  1.1503e+00,  1.1225e+00,\n",
       "          1.1341e+00,  1.1080e+00,  1.1862e+00,  1.0400e+00,  7.9060e-01,\n",
       "          4.9317e-01,  1.7960e-01,  3.0880e-02, -2.3564e-01, -2.6669e-01,\n",
       "         -3.2111e-01, -5.2646e-01, -6.9474e-01, -6.0520e-01, -6.8154e-01,\n",
       "         -5.8691e-01, -5.0137e-01, -5.8462e-01, -6.7569e-01, -7.1291e-01,\n",
       "         -5.2389e-01, -6.7361e-01, -6.8915e-01, -6.7107e-01, -6.3491e-01,\n",
       "          1.5934e-02, -7.5042e-02,  1.8745e-01, -2.5574e-02, -1.7556e-01,\n",
       "         -4.8608e-01, -3.6368e-01, -2.2600e-01, -1.1137e-01, -1.4632e-01,\n",
       "          5.3003e-02,  1.3596e-02, -1.1677e-01,  3.2733e-02, -1.4719e-01,\n",
       "          1.5841e-01, -1.4234e-01,  3.1101e-02,  2.0234e-01, -4.7224e-03,\n",
       "          1.1410e-01,  8.1758e-02, -9.2741e-03,  7.0269e-02,  1.8011e-01,\n",
       "          3.7053e-02, -2.3010e-02, -1.1524e-01, -1.5297e-02, -1.1402e-01,\n",
       "         -9.2418e-02,  3.6270e-03, -6.1549e-02, -1.4952e-01, -1.4128e-01,\n",
       "         -3.3516e-01, -2.2860e-01, -3.5717e-01, -3.6198e-01, -3.7170e-01,\n",
       "         -3.0876e-01, -2.9965e-01, -3.3459e-01, -2.9047e-01, -1.2022e-01,\n",
       "         -9.1141e-02, -1.9733e-01, -1.2208e-01, -1.6454e-01, -8.9366e-02,\n",
       "         -8.9160e-02, -9.8955e-02, -1.1654e-01, -8.0740e-02,  7.6455e-02,\n",
       "         -1.3196e-02, -1.0105e-01, -1.9887e-01, -1.0147e-02, -3.2503e-02,\n",
       "          1.3741e+00,  1.4414e+00,  1.4770e+00,  1.5562e+00,  1.5005e+00,\n",
       "          1.5286e+00,  1.6232e+00,  1.6248e+00,  1.4868e+00,  1.4979e+00,\n",
       "          1.4367e+00,  1.4683e+00,  1.4359e+00,  1.4333e+00,  1.3966e+00,\n",
       "          1.4089e+00,  1.3451e+00,  1.2995e+00,  1.4017e+00,  1.5150e+00,\n",
       "          1.3571e+00,  1.3920e+00,  1.3884e+00,  1.2694e+00,  1.3871e+00,\n",
       "          1.3958e+00,  1.3765e+00,  1.4146e+00,  1.5465e+00,  1.5084e+00,\n",
       "          1.4820e+00,  1.3240e+00,  1.6390e+00,  1.5625e+00,  1.5779e+00,\n",
       "          1.7243e+00,  1.6506e+00,  1.6833e+00,  1.6701e+00,  1.3789e+00,\n",
       "          9.2609e-01,  6.0186e-01,  2.6189e-01,  1.7648e-01, -1.1015e-01,\n",
       "          2.6061e-02, -1.7670e-01, -3.7635e-01, -3.4224e-01, -3.7544e-01,\n",
       "         -3.0693e-01, -3.4084e-01, -5.2724e-01, -5.4506e-01, -4.9160e-01,\n",
       "         -5.0202e-01, -4.6633e-01, -3.5276e-01, -4.7347e-01, -5.1093e-01,\n",
       "         -1.5638e-01, -2.5202e-01, -1.2026e-01, -1.2356e-01, -2.8510e-01,\n",
       "         -6.6295e-01, -5.9347e-01, -1.8618e-01, -1.0999e-01,  5.8889e-03,\n",
       "         -5.7296e-02, -5.9221e-02, -1.7340e-01,  8.4569e-02, -2.5752e-02,\n",
       "         -1.2733e-01,  2.5261e-02, -5.0636e-02, -5.0569e-02, -1.0237e-01,\n",
       "          1.4045e-01, -1.1694e-01, -2.0361e-02, -7.9196e-02, -1.4070e-01,\n",
       "         -1.2032e-01, -1.3749e-01,  5.3715e-03, -2.2364e-01, -2.7924e-02,\n",
       "         -1.2281e-01, -5.9741e-02, -1.1588e-01, -2.9810e-01, -2.4940e-01,\n",
       "         -3.6089e-01, -4.3179e-01, -4.6262e-01, -5.7360e-01, -7.2083e-01,\n",
       "         -7.3341e-01, -4.7027e-01, -5.7401e-01, -5.0345e-01, -6.2648e-01,\n",
       "         -3.7896e-01, -4.0772e-01, -4.6105e-01, -3.6895e-01, -4.2185e-01,\n",
       "         -3.1253e-01, -3.9260e-01, -4.6149e-01, -4.8092e-01, -3.2711e-01,\n",
       "         -2.6207e-01, -4.3893e-01, -3.2226e-01, -3.7903e-01, -1.3306e-01,\n",
       "          6.7950e-01,  7.5816e-01,  6.2904e-01,  7.1093e-01,  5.3564e-01,\n",
       "          5.1210e-01,  4.1507e-01,  6.4947e-01,  5.3505e-01,  5.1485e-01,\n",
       "          6.0148e-01,  6.7333e-01,  6.8978e-01,  5.4900e-01,  6.9650e-01,\n",
       "          6.3011e-01,  6.3677e-01,  4.7182e-01,  6.4328e-01,  7.9303e-01,\n",
       "          8.7076e-01,  6.8555e-01,  7.8487e-01,  6.2602e-01,  5.6076e-01,\n",
       "          7.0204e-01,  7.2599e-01,  6.8064e-01,  7.1047e-01,  7.2120e-01,\n",
       "          7.2360e-01,  6.1049e-01,  6.0099e-01,  6.0959e-01,  5.4337e-01,\n",
       "          5.4587e-01,  6.5258e-01,  5.3449e-01,  3.5343e-01,  3.9739e-01,\n",
       "          3.3245e-02, -5.7561e-02, -3.0952e-01, -2.0836e-01, -2.3287e-01,\n",
       "         -2.7025e-01, -4.0805e-01, -3.1244e-01, -3.8482e-01, -5.3863e-01,\n",
       "         -3.2630e-01, -3.2121e-01, -4.2974e-01, -5.3379e-01, -4.1585e-01,\n",
       "         -5.0788e-01, -4.1288e-01, -5.4838e-01, -3.7659e-01, -4.0358e-01,\n",
       "         -7.1747e-01, -8.0575e-02,  1.6153e-01, -1.3750e-01,  2.6019e-01,\n",
       "          2.9090e-01,  5.6590e-01,  6.4102e-02],\n",
       "        [ 2.6452e-01,  4.9604e-01,  3.5396e-01,  2.6182e-01,  4.6015e-01,\n",
       "          3.4458e-01,  2.9920e-01,  2.2962e-01,  2.1288e-01,  4.6690e-01,\n",
       "          3.3574e-01,  3.1075e-01,  2.9499e-01,  2.5971e-01,  3.8999e-01,\n",
       "          2.1984e-01,  2.5275e-01,  2.3065e-01,  2.3560e-01,  2.8006e-01,\n",
       "          2.6392e-01,  1.9585e-01,  3.6618e-01,  3.3671e-01,  2.8692e-01,\n",
       "          4.4870e-01,  2.4677e-01,  4.0113e-01,  3.8573e-01,  3.2413e-01,\n",
       "          3.8029e-01,  3.5494e-01,  2.0727e-01,  3.5780e-01,  3.2761e-01,\n",
       "          4.6074e-01,  5.6905e-01,  4.5730e-01,  6.0788e-01,  7.9059e-01,\n",
       "          8.3479e-01,  1.0107e+00,  1.0194e+00,  7.3321e-01,  8.2677e-01,\n",
       "          6.5093e-01,  7.1376e-01,  6.1892e-01,  5.8778e-01,  4.7083e-01,\n",
       "          5.2823e-01,  5.2729e-01,  3.3343e-01,  4.4113e-01,  2.7109e-01,\n",
       "          3.2463e-01,  2.5246e-01,  3.9755e-01,  2.1140e-01,  1.7623e-01,\n",
       "          1.0611e+00,  1.2448e+00,  1.2138e+00,  1.1229e+00,  9.8546e-01,\n",
       "          1.0530e+00,  1.1465e+00,  9.9941e-01,  1.1087e+00,  9.8962e-01,\n",
       "          1.0409e+00,  1.0806e+00,  1.0987e+00,  1.0360e+00,  9.4898e-01,\n",
       "          1.0140e+00,  9.4037e-01,  8.9003e-01,  1.0226e+00,  1.0546e+00,\n",
       "          1.1442e+00,  1.0787e+00,  1.0913e+00,  1.0294e+00,  9.3826e-01,\n",
       "          9.7566e-01,  1.0694e+00,  1.1457e+00,  1.1964e+00,  1.0268e+00,\n",
       "          1.1010e+00,  1.1977e+00,  1.1758e+00,  1.2830e+00,  1.1767e+00,\n",
       "          1.0894e+00,  1.1794e+00,  1.2102e+00,  1.1899e+00,  1.0347e+00,\n",
       "          9.6684e-01,  6.5786e-01,  4.7423e-01,  5.1344e-01,  2.8645e-01,\n",
       "         -9.8340e-02, -1.5889e-01, -2.9013e-01, -4.2858e-01, -5.0011e-01,\n",
       "         -3.7784e-01, -5.0462e-01, -5.4648e-01, -5.5181e-01, -7.2286e-01,\n",
       "         -7.0590e-01, -6.7207e-01, -7.0526e-01, -6.4770e-01, -7.5255e-01,\n",
       "          3.7954e-02, -4.8740e-02, -5.0564e-02,  7.0545e-02, -2.3456e-01,\n",
       "         -2.3716e-01, -1.3855e-01, -2.6295e-03, -7.5900e-02, -1.7870e-01,\n",
       "          5.7228e-02,  4.8872e-02,  3.2456e-02,  1.1198e-01,  4.7501e-02,\n",
       "          1.0784e-01, -1.1538e-02,  9.2059e-02, -8.2745e-02,  1.1300e-01,\n",
       "         -1.6603e-02,  1.1728e-01, -2.5953e-02,  9.4146e-02,  1.6998e-02,\n",
       "          7.3456e-02,  3.4130e-02, -1.6318e-02, -8.2080e-02, -1.1223e-01,\n",
       "         -8.0847e-02, -4.0630e-02, -1.9175e-01, -2.2630e-01, -1.8562e-01,\n",
       "         -2.3749e-01, -2.3054e-01, -2.7668e-01, -1.7035e-01, -4.3790e-01,\n",
       "         -4.2993e-01, -2.8888e-01, -4.1592e-01, -3.7611e-01, -3.1813e-01,\n",
       "         -2.4951e-01, -2.4516e-01, -3.0003e-01, -2.0528e-01, -2.9119e-01,\n",
       "         -7.3527e-02, -7.5064e-02, -2.7156e-01, -2.0736e-01, -6.6312e-02,\n",
       "         -9.4317e-03, -7.8501e-02, -8.1308e-02, -2.4192e-02, -1.9952e-02,\n",
       "          1.3504e+00,  1.3738e+00,  1.5073e+00,  1.5146e+00,  1.5531e+00,\n",
       "          1.5372e+00,  1.5868e+00,  1.5093e+00,  1.4219e+00,  1.5301e+00,\n",
       "          1.5624e+00,  1.4382e+00,  1.4543e+00,  1.4536e+00,  1.4532e+00,\n",
       "          1.4480e+00,  1.3965e+00,  1.4033e+00,  1.3166e+00,  1.2166e+00,\n",
       "          1.3731e+00,  1.3118e+00,  1.3805e+00,  1.4472e+00,  1.2957e+00,\n",
       "          1.4032e+00,  1.3848e+00,  1.3880e+00,  1.5684e+00,  1.3523e+00,\n",
       "          1.3643e+00,  1.4701e+00,  1.3920e+00,  1.3356e+00,  1.4111e+00,\n",
       "          1.5620e+00,  1.6079e+00,  1.5568e+00,  1.5165e+00,  1.6086e+00,\n",
       "          1.4918e+00,  1.1952e+00,  9.8420e-01,  9.7776e-01,  6.3908e-01,\n",
       "          3.5201e-01,  2.0846e-01,  3.4968e-02, -6.9193e-02, -1.5641e-01,\n",
       "         -1.9890e-01, -3.3865e-01, -3.6578e-01, -4.9227e-01, -5.4720e-01,\n",
       "         -4.8916e-01, -7.1427e-01, -5.9813e-01, -6.0212e-01, -7.0864e-01,\n",
       "         -2.2587e-02, -3.0043e-02, -1.0775e-01, -3.9075e-02, -3.2837e-01,\n",
       "         -2.4586e-01, -1.6771e-01, -1.6017e-01, -9.6915e-03, -6.5887e-02,\n",
       "         -1.4235e-01, -5.6875e-02, -9.2855e-02,  4.3934e-03,  9.0700e-02,\n",
       "         -3.1355e-02, -1.7665e-02, -6.3204e-02,  5.8240e-02, -9.0475e-03,\n",
       "         -1.7890e-02,  3.1727e-02, -1.4055e-02,  1.0111e-01,  6.9754e-04,\n",
       "          4.1962e-02, -8.6589e-02, -8.8384e-02, -1.0792e-02, -1.5033e-01,\n",
       "         -1.1328e-01, -1.8352e-01, -1.6211e-01, -1.3272e-01, -1.6316e-01,\n",
       "         -2.4151e-01, -2.2528e-01, -3.6993e-01, -2.7019e-01, -3.7475e-01,\n",
       "         -5.5024e-01, -3.9728e-01, -4.5794e-01, -3.1053e-01, -4.7146e-01,\n",
       "         -4.7919e-01, -3.8716e-01, -3.8141e-01, -2.4098e-01, -3.2783e-01,\n",
       "         -4.1370e-01, -4.6620e-01, -3.3367e-01, -3.2210e-01, -4.1527e-01,\n",
       "         -2.8816e-01, -5.0635e-01, -3.8107e-01, -2.8668e-01, -3.4025e-01,\n",
       "          7.4512e-01,  7.7097e-01,  7.7941e-01,  6.8523e-01,  7.0348e-01,\n",
       "          7.2596e-01,  8.2247e-01,  7.2666e-01,  5.5866e-01,  6.2078e-01,\n",
       "          6.5668e-01,  7.4929e-01,  5.0477e-01,  8.4353e-01,  6.8247e-01,\n",
       "          6.7833e-01,  6.3504e-01,  6.5476e-01,  7.2175e-01,  7.1065e-01,\n",
       "          5.7723e-01,  5.3534e-01,  6.3896e-01,  8.3068e-01,  7.5690e-01,\n",
       "          9.0108e-01,  7.6545e-01,  6.4841e-01,  7.7115e-01,  8.2046e-01,\n",
       "          7.1726e-01,  7.1661e-01,  6.4870e-01,  7.6395e-01,  6.7942e-01,\n",
       "          6.5491e-01,  6.6895e-01,  6.7777e-01,  7.6730e-01,  6.3843e-01,\n",
       "          6.2621e-01,  3.5297e-01,  3.6766e-01,  1.6229e-01, -6.5329e-02,\n",
       "          7.0062e-02, -1.6291e-01, -1.1015e-01, -1.5148e-01, -1.3027e-01,\n",
       "         -2.7669e-01, -3.6168e-01, -3.3029e-01, -3.1514e-01, -3.6604e-01,\n",
       "         -3.2252e-01, -2.9364e-01, -4.9747e-01, -4.1159e-01, -3.5381e-01,\n",
       "         -7.0341e-01, -2.8522e-02,  5.0014e-02, -4.1399e-02,  3.3737e-01,\n",
       "          4.7595e-01,  6.1660e-01,  6.0418e-02]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEADModelAtt(**nn_config)(torch.cat([LEAD_Dataset(df).__getitem__(3)[0].view(1, -1), LEAD_Dataset(df).__getitem__(5)[0].view(1, -1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LEAD_Dataset(df).__getitem__(63)[1]\n",
    "b = LEAD_Dataset(df).__getitem__(9)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([368])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1720, dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MSELoss()(torch.tensor(np.expand_dims(a, 0)), torch.tensor(np.expand_dims(b, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y_pred:torch.Tensor, y_true:torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the R^2 (coefficient of determination) regression score.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : torch.Tensor\n",
    "        The predicted values.\n",
    "    y_true : torch.Tensor\n",
    "        The true values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The R^2 score, a float value.\n",
    "    \"\"\"\n",
    "    #y_true = y_true * torch.tensor(TARGET_WEIGHTS)\n",
    "    #y_pred = y_pred * torch.tensor(TARGET_WEIGHTS)\n",
    "\n",
    "    \n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    \n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    \n",
    "    return r2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "#from torcheval.metrics import R2Score \n",
    "from torchmetrics.regression import R2Score\n",
    "metric = R2Score()\n",
    "\n",
    "\n",
    "\n",
    "class LEADModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        self.backbone = LEADModelAtt(**nn_config).to(config.DEVICE)\n",
    "        \n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.metric = r2_score\n",
    "        #self.metric = R2Score()\n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        # == define learning rate scheduler ==\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(\n",
    "            model_optimizer,\n",
    "            T_0=config.EPOCHS,\n",
    "            T_mult=1,\n",
    "            eta_min=1e-7,\n",
    "            last_epoch=-1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device).float()\n",
    "        target = target.to(self.device).float()\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch\n",
    "        image = image.to(self.device).float()\n",
    "        target = target.to(self.device).float()\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_epoch_start(self):\n",
    "        print('\\n')\n",
    "\n",
    "    def on_load_checkpoint(self, checkpoint: dict) -> None:\n",
    "        state_dict = checkpoint[\"state_dict\"]\n",
    "        model_state_dict = self.state_dict()\n",
    "        is_changed = False\n",
    "        for k in state_dict:\n",
    "            if k in model_state_dict:\n",
    "                if state_dict[k].shape != model_state_dict[k].shape:\n",
    "                    print(f\"Skip loading parameter: {k}, \"\n",
    "                                f\"required shape: {model_state_dict[k].shape}, \"\n",
    "                                f\"loaded shape: {state_dict[k].shape}\")\n",
    "                    state_dict[k] = model_state_dict[k]\n",
    "                    is_changed = True\n",
    "            else:\n",
    "                print(f\"Dropping parameter {k}\")\n",
    "                is_changed = True\n",
    "\n",
    "        if is_changed:\n",
    "            checkpoint.pop(\"optimizer_states\", None)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        #output_val = nn.Sigmoid()(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        #output_val = torch.cat([x['logits'] for x in outputs], dim=0).cpu().detach()\n",
    "        #target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        output_val = torch.cat([x['logits'] for x in outputs], dim=0)#.cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0)#.cpu().detach()\n",
    "        \n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        # == record ==\n",
    "        print(f\"val_loss: {val_loss}\")\n",
    "        self.log('val_loss', val_loss, True)\n",
    "        \n",
    "        val_loss = val_loss.cpu().detach()\n",
    "\n",
    "    \n",
    "        #output_val = nn.Sigmoid()(output_val).cpu().detach()\n",
    "        output_val = output_val.view(-1,368).cpu().detach()\n",
    "        target_val = target_val.view(-1,368).cpu().detach()\n",
    "\n",
    "  \n",
    "        y = (output_val * std_y) + mean_y\n",
    "        \n",
    "        y_pred = target_val\n",
    "        y_pred[:, std_y < 1e-9] = 0\n",
    "        y_pred = (y_pred * std_y) + mean_y\n",
    "\n",
    "        # r2=0\n",
    "        # for i in range(368):\n",
    "        #     r2_i = self.metric(y_pred[:, i], y[:, i])\n",
    "        #     r2 += r2_i\n",
    "        # val_score  = r2/ 368\n",
    "        # #val_score = self.metric(y_pred, y)\n",
    "\n",
    "        val_score = self.metric(y_pred, y)\n",
    "        \n",
    "        # r2=0\n",
    "        # delim = 0\n",
    "        # for i in range(368):\n",
    "        #     r2_i = self.metric(y_pred[:, i], y[:, i])\n",
    "        #     if r2_i > 1e-6:\n",
    "        #         r2 += r2_i\n",
    "        #         delim += 1\n",
    "        # val_score  = r2/ delim\n",
    "        \n",
    "        \n",
    "        # self.metric.update(target_val, output_val)\n",
    "        # val_score = self.metric.compute()\n",
    "        \n",
    "        # target to one-hot\n",
    "        #target_val = torch.nn.functional.one_hot(target_val, len(label_list))\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        # gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        # pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        # gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        # pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        # val_score = score(gt_df.drop(cols_drop_on_val, axis=1), pred_df.drop(cols_drop_on_val, axis=1), row_id_column_name='id')\n",
    "        \n",
    "        print(f\"val_R2: {val_score}\")\n",
    "        \n",
    "        self.log(\"val_R2\", val_score, True)\n",
    "        \n",
    "        # clear validation outputs\n",
    "        self.validation_step_outputs = list()\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_R2': val_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CHECKPOINT = False\n",
    "#CHK_PATH = './pretrain_checkpoints/eca_nfnet_l0_fold_0_0.97126.ckpt'\n",
    "\n",
    "\n",
    "def run_training(fold_id, total_df):\n",
    "    print('================================================================')\n",
    "    print(f\"==== Running training for fold {fold_id} ====\")\n",
    "    \n",
    "    # == create dataset and dataloader ==\n",
    "    train_df = total_df[total_df['fold'] != fold_id].drop('fold', axis=1).copy()\n",
    "    valid_df = total_df[total_df['fold'] == fold_id].drop('fold', axis=1).copy()\n",
    "    \n",
    "    print(f'Train Samples: {len(train_df)}')\n",
    "    print(f'Valid Samples: {len(valid_df)}')\n",
    "    \n",
    "  \n",
    "    train_ds = LEAD_Dataset(train_df)\n",
    "    val_ds =  LEAD_Dataset(valid_df)\n",
    "    #val_ds = WaveAllFileDataset(df=valid_df, name_col=\"filepath\", **val_dataset_config)\n",
    "    \n",
    "    \n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        #num_workers=config.N_WORKERS,\n",
    "        pin_memory=True,\n",
    "        #persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=config.BATCH_SIZE * 2,\n",
    "        shuffle=False,\n",
    "        #num_workers=config.N_WORKERS,\n",
    "        pin_memory=True,\n",
    "        #persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # == init model ==\n",
    "    if USE_CHECKPOINT:\n",
    "        model = LEADModel.load_from_checkpoint(CHK_PATH, strict=False)\n",
    "    else:\n",
    "        model = LEADModel()\n",
    "    # == init callback ==\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss',\n",
    "                                          dirpath=config.OUTPUT_DIR,\n",
    "                                          save_top_k=1,\n",
    "                                          save_last=True,\n",
    "                                          save_weights_only=True,\n",
    "                                          filename=f\"fold_{fold_id}\",\n",
    "                                          mode='min')\n",
    "\n",
    "    callbacks_to_use = [checkpoint_callback, TQDMProgressBar(refresh_rate=1)]\n",
    "\n",
    "    print(f'trainer')\n",
    "    # == init trainer ==\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.EPOCHS,\n",
    "        val_check_interval=1.,\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=callbacks_to_use,\n",
    "        enable_model_summary=False,\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else 'auto',\n",
    "        deterministic=True,\n",
    "        precision='16-mixed' if config.MIXED_PRECISION else 32,\n",
    "    )\n",
    "    \n",
    "    # == Training ==\n",
    "    trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)\n",
    "    \n",
    "    # == Prediction ==\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    weights = torch.load(best_model_path)['state_dict']\n",
    "    model.load_state_dict(weights)\n",
    "    \n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.target<30].reset_index(drop=True)\n",
    "\n",
    "kf = KFold(n_splits=config.FOLDS, shuffle=True, random_state=config.SEED)\n",
    "df['fold'] = 0\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(df)):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.EPOCHS = 10\n",
    "#config.LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "def disable_logging_during_tests():\n",
    "    # Store the current log level to restore it later\n",
    "    original_log_level = logging.getLogger().getEffectiveLevel()\n",
    "\n",
    "    # Set the log level to a higher level, e.g., WARNING or CRITICAL\n",
    "    logging.disable(logging.ERROR)\n",
    "\n",
    "    # Run your tests here\n",
    "\n",
    "    # Restore the original log level after the tests\n",
    "    logging.disable(original_log_level)\n",
    "\n",
    "# Call this function before running your tests\n",
    "disable_logging_during_tests()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "==== Running training for fold 0 ====\n",
      "Train Samples: 171428\n",
      "Valid Samples: 28572\n",
      "trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory E:\\PycharmProjects\\LEAP\\output exists and is not empty.\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04c5ed917c44d04a5cf393094086574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.5748037695884705\n",
      "val_R2: -0.8353511652778718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.44946566224098206\n",
      "val_R2: -0.2161903721419367\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.4143925607204437\n",
      "val_R2: 0.07491585129627798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "==== Running training for fold 4 ====\n",
      "Train Samples: 171429\n",
      "Valid Samples: 28571\n",
      "trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:653: Checkpoint directory E:\\PycharmProjects\\LEAP\\output exists and is not empty.\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "E:\\PycharmProjects\\birdclef24\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615b53ca6f9d4fc39c3f833ce2757cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_folds = [0,4,5]\n",
    "    \n",
    "# training\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "\n",
    "\n",
    "for f in range(config.FOLDS):\n",
    "    \n",
    "    if f not in selected_folds:\n",
    "        continue\n",
    "    \n",
    "    # get validation index\n",
    "    #val_idx = list(train_df[train_df['fold'] == f].index)\n",
    "    \n",
    "    # main loop of f-fold\n",
    "    trainer = run_training(f, df)\n",
    "    \n",
    "\n",
    "    \n",
    "    # only training one fold\n",
    "    #break\n",
    "\n",
    "\n",
    "# for idx, val_score in enumerate(fold_val_score_list):\n",
    "#     print(f'Fold {idx} Val Score: {val_score:.5f}')\n",
    "\n",
    "# oof_gt_df = oof_df[['samplename'] + label_list].copy()\n",
    "# oof_pred_df = oof_df[['samplename'] + pred_cols].copy()\n",
    "# oof_pred_df.columns = ['samplename'] + label_list\n",
    "# oof_score = score(oof_gt_df, oof_pred_df, 'samplename')\n",
    "# print(f'OOF Score: {oof_score:.5f}')\n",
    "\n",
    "#oof_df.to_csv(f\"{config.OUTPUT_DIR}/oof_pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(tokenized, split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LEADModelAtt(**nn_config).to(config.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "        LEAD_Dataset(df.drop('fold', axis=1)),\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        #persistent_workers=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 2000\n",
    "eval_iters = 1000\n",
    "learning_rate = 3e-4\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for step in range(max_iters):\n",
    "    #print(iter)\n",
    "    # if iter % eval_iters == 0:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    #xb, yb = LEAD_Dataset(df.drop('fold', axis=1)).__getitem__(iter)\n",
    "\n",
    "    xb, yb = next(iter(train_dl))\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits = model.forward(xb.to('cuda').float(), yb.to('cuda').float())\n",
    "\n",
    "    loss = nn.MSELoss()(logits, yb.to('cuda').float())\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        y = (yb * std_y) + mean_y\n",
    "        \n",
    "        y_pred = logits.cpu().detach()\n",
    "        y_pred[:, std_y < 1e-9] = 0\n",
    "        y_pred = (y_pred * std_y) + mean_y\n",
    "\n",
    "        # r2=0\n",
    "        # for i in range(368):\n",
    "        #     r2_i = self.metric(y_pred[:, i], y[:, i])\n",
    "        #     r2 += r2_i\n",
    "        # val_score  = r2/ 368\n",
    "        r2_score(y_pred.view(-1, 368, 1), y.view(-1, 368, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred*torch.tensor(TARGET_WEIGHTS)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.EPOCHS = 25  # max epochs\n",
    "#config.LR = 3e-4  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/version_0/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2091745,
     "sourceId": 25954,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 3221581,
     "sourceId": 33246,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 5188730,
     "sourceId": 44224,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 1292430,
     "sourceId": 19596,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 8508280,
     "datasetId": 4979005,
     "sourceId": 8374380,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8528074,
     "datasetId": 4990143,
     "sourceId": 8393186,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 5267533,
     "datasetId": 3020983,
     "sourceId": 5195317,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8207598,
     "datasetId": 4776799,
     "sourceId": 8090934,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 8496617,
     "datasetId": 4970788,
     "sourceId": 8363310,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 152757,
     "datasetId": 3151,
     "sourceId": 142598,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 3680898,
     "datasetId": 2172852,
     "sourceId": 3627245,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 1520932,
     "datasetId": 726237,
     "sourceId": 1487019,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 1521030,
     "datasetId": 726312,
     "sourceId": 1487116,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 154204277,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 167220511,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
